{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89b6e6fc",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70be667f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-29 10:49:51.108022: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#arrays \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# plots\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ML\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression,Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV,train_test_split,cross_val_score,cross_validate\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score,roc_curve, auc, brier_score_loss\n",
    "from sklearn import tree\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pydotplus\n",
    "import graphviz\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from keras.layers import Dense, Input, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Activation, Embedding\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "#NN\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef49e32",
   "metadata": {},
   "source": [
    "### Helper Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41939583",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def y_roc(estimator, X):\n",
    "        return [item[1] for item in estimator.predict(X)]\n",
    "\n",
    "    @staticmethod\n",
    "    def y_roc_regression(estimator, X):\n",
    "        return estimator.predict(X).tolist()\n",
    "\n",
    "    def cv_roc_plot(self, estimator, X, y):\n",
    "        cv = StratifiedKFold(n_splits=4, shuffle=False)\n",
    "        tprs = []\n",
    "        aucs = []\n",
    "        mean_fpr = np.linspace(0, 1, 100)\n",
    "        for train, test in cv.split(X, y):\n",
    "            prediction = estimator.fit(X.iloc[train], y.iloc[train]).predict(X.iloc[test])\n",
    "            fpr, tpr, t = roc_curve(y.iloc[test], prediction[:, 1])\n",
    "            tpr[0] = 0\n",
    "            tpr[-1] = 1\n",
    "            tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            aucs.append(roc_auc)\n",
    "        mean_tpr = np.mean(tprs, axis=0)\n",
    "        mean_auc = auc(mean_fpr, mean_tpr)\n",
    "        return mean_fpr, mean_tpr, mean_auc\n",
    "\n",
    "    @staticmethod\n",
    "    def brier_score(y_prob_raw, y_true):\n",
    "        y_prob = [prob[1] for prob in y_prob_raw]\n",
    "        if len(y_prob) != len(y_true):\n",
    "            print('Error: two lists must have same length')\n",
    "            return\n",
    "        return np.mean([(prob_1 - y) ** 2 for prob_1, y in zip(y_prob, y_true)])\n",
    "\n",
    "    @staticmethod\n",
    "    def get_prob_1(y_prob_raw):\n",
    "        return [prob[1] for prob in y_prob_raw]\n",
    "\n",
    "    @staticmethod\n",
    "    def aver_prob(prob_lists):\n",
    "        return np.mean(np.array(prob_lists), axis=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_confusion_matrix(matrix, title=\"\"):\n",
    "        sns.heatmap(data=matrix, annot=True, cmap=\"spring\")\n",
    "        status = [\"0\", \"1\"]\n",
    "        axis_ticks = np.arange(len(status)) + 0.4\n",
    "        plt.xticks(axis_ticks, status)\n",
    "        plt.yticks(axis_ticks, status)\n",
    "        plt.title(title)\n",
    "        plt.ylabel(\"Actual Label\")\n",
    "        plt.xlabel(\"Predicted Label\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872ba049",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ee42f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/puddinpop/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/puddinpop/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "class RecidivismDataPreprocessor:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.recidivism = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "\n",
    "    def read_data(self):\n",
    "        self.recidivism = pd.read_csv(self.url)\n",
    "\n",
    "    def drop_columns(self, cols_2_drop):\n",
    "        self.recidivism = self.recidivism.drop(cols_2_drop, axis=1)\n",
    "\n",
    "    def preprocess_columns(self):\n",
    "        self.recidivism['Residence_PUMA'] = self.recidivism['Residence_PUMA'].astype('category').cat.codes\n",
    "        self.recidivism['Age_at_Release'] = self.recidivism['Age_at_Release'].apply(lambda x: int(x[:2]))\n",
    "        self.recidivism['Dependents'] = self.recidivism['Dependents'].apply(lambda x: int(x[:1]))\n",
    "        self.recidivism['Prior_Arrest_Episodes_Felony'] = self.recidivism['Prior_Arrest_Episodes_Felony'].apply(lambda x: int(x[:2]))\n",
    "        self.recidivism['Prior_Arrest_Episodes_Drug'] = self.recidivism['Prior_Arrest_Episodes_Drug'].apply(lambda x: int(x[:2]))\n",
    "        self.recidivism['Prior_Arrest_Episodes_Misd']=self.recidivism['Prior_Arrest_Episodes_Misd'].apply(lambda x: int(x[:2]))\n",
    "        self.recidivism['Prior_Arrest_Episodes_Violent']=self.recidivism['Prior_Arrest_Episodes_Violent'].apply(lambda x: int(x[:2]))\n",
    "        self.recidivism['Prior_Arrest_Episodes_Property']=self.recidivism['Prior_Arrest_Episodes_Property'].apply(lambda x: int(x[:2]))\n",
    "        self.recidivism['Prior_Arrest_Episodes_PPViolationCharges']=self.recidivism['Prior_Arrest_Episodes_PPViolationCharges'].apply(lambda x: int(x[:2]))\n",
    "        self.recidivism['Prior_Conviction_Episodes_Felony']=self.recidivism['Prior_Conviction_Episodes_Felony'].apply(lambda x: int(x[:1]))\n",
    "        self.recidivism['Prior_Conviction_Episodes_Misd']=self.recidivism['Prior_Conviction_Episodes_Misd'].apply(lambda x: int(x[:1]))\n",
    "        self.recidivism['Prior_Conviction_Episodes_Prop']=self.recidivism['Prior_Conviction_Episodes_Prop'].apply(lambda x: int(x[:1]))\n",
    "        self.recidivism['Prior_Conviction_Episodes_Drug']=self.recidivism['Prior_Conviction_Episodes_Drug'].apply(lambda x: int(x[:1]))\n",
    "        self.recidivism['Delinquency_Reports']=self.recidivism['Delinquency_Reports'].apply(lambda x: int(x[:1]))\n",
    "        self.recidivism['Program_Attendances']=self.recidivism['Program_Attendances'].apply(lambda x: int(x[:2]))\n",
    "        self.recidivism['Program_UnexcusedAbsences']=self.recidivism['Program_UnexcusedAbsences'].apply(lambda x: int(x[:1]))\n",
    "        self.recidivism['Residence_Changes']=self.recidivism['Residence_Changes'].apply(lambda x: int(x[:1]))\n",
    "\n",
    "    def scale_columns(self):\n",
    "        scaler = StandardScaler()\n",
    "        scaling_set = [column for column in self.recidivism.columns if self.recidivism[column].dtype in ['int64', 'float32', 'float64']]\n",
    "        self.recidivism[scaling_set] = scaler.fit_transform(self.recidivism[scaling_set].values)\n",
    "        self.recidivism.update(self.recidivism.select_dtypes(include=[object]).astype('category').apply(lambda x: x.cat.codes))\n",
    "\n",
    "    def remove_outliers(self):\n",
    "        self.recidivism = self.recidivism.dropna(subset=['Supervision_Risk_Score_First', 'Prison_Offense']).reset_index(drop=True)\n",
    "\n",
    "    def impute_missing_values(self, columns_to_impute):\n",
    "        for missing_column in columns_to_impute:\n",
    "            test_index = self.recidivism[self.recidivism[missing_column] == -1].index\n",
    "            train_index = self.recidivism[self.recidivism[missing_column] != -1].index\n",
    "            X = self.recidivism.drop(columns=[missing_column])\n",
    "            y = self.recidivism[missing_column].astype('int64')  # Ensure y is an integer type\n",
    "\n",
    "            logreg = LogisticRegression()\n",
    "            logreg.fit(X.iloc[train_index, :], y[train_index])\n",
    "            self.recidivism.loc[test_index, missing_column] = logreg.predict(X.iloc[test_index, :])\n",
    "            self.recidivism[missing_column] = self.recidivism[missing_column].astype('category').cat.codes\n",
    "\n",
    "\n",
    "    def split_data(self, test_size=0.33, random_state=42):\n",
    "        features = self.recidivism.drop(columns=[\"Recidivism_Within_3years\"])\n",
    "        target = self.recidivism.Recidivism_Within_3years\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            features, target, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    def convert_to_numpy(self):\n",
    "        self.X_train = self.X_train.values\n",
    "        self.X_test = self.X_test.values\n",
    "        self.y_train = self.y_train.values\n",
    "        self.y_test = self.y_test.values\n",
    "\n",
    "    def preprocess(self):\n",
    "        self.read_data()\n",
    "        self.drop_columns(cols_2_drop)\n",
    "        self.preprocess_columns()\n",
    "        self.scale_columns()\n",
    "        self.remove_outliers()\n",
    "        self.impute_missing_values(['Supervision_Level_First', 'Prison_Offense'])\n",
    "        self.split_data()\n",
    "        self.convert_to_numpy()\n",
    "\n",
    "# Usage example:\n",
    "url = 'https://raw.githubusercontent.com/bswain7/MLP_Recidivism/nij_data/NIJ_s_Recidivism_Challenge_Full_Dataset.csv'\n",
    "cols_2_drop =['Violations_ElectronicMonitoring',\n",
    "       'Violations_Instruction', 'Violations_FailToReport',\n",
    "       'Violations_MoveWithoutPermission',\n",
    "       'Avg_Days_per_DrugTest', 'DrugTests_THC_Positive',\n",
    "       'DrugTests_Cocaine_Positive', 'DrugTests_Meth_Positive',\n",
    "       'DrugTests_Other_Positive', 'Percent_Days_Employed', 'Jobs_Per_Year',\n",
    "       'Employment_Exempt']\n",
    "\n",
    "data_preprocessor = RecidivismDataPreprocessor(url)\n",
    "data_preprocessor.preprocess()\n",
    "\n",
    "# Access the preprocessed data\n",
    "X_train = data_preprocessor.X_train\n",
    "X_test = data_preprocessor.X_test\n",
    "y_train = data_preprocessor.y_train\n",
    "y_test = data_preprocessor.y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52c0ab5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9915745620139348, 0, 1, ..., False, False, 0.6580642757972365],\n",
       "       [-1.6671730149464654, 1, 0, ..., False, False,\n",
       "        -1.5196083981135624],\n",
       "       [-0.16809055666905115, 1, 1, ..., False, False,\n",
       "        0.6580642757972365],\n",
       "       ...,\n",
       "       [-1.6088983858669184, 0, 1, ..., False, False, 0.6580642757972365],\n",
       "       [0.42009469950717687, 1, 0, ..., False, False, 0.6580642757972365],\n",
       "       [1.5010243193449082, 1, 0, ..., True, False, 0.6580642757972365]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78f1a6c",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09407206",
   "metadata": {},
   "source": [
    "##### Multi Layer Perceptron with Mean Sqaured Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a2a5f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel:\n",
    "    def __init__(self, X_train, y_train, X_test, y_test):\n",
    "        self.X_train = X_train.fillna(0).astype('float32')\n",
    "        self.y_train = y_train.fillna(0)\n",
    "        self.X_test = X_test.fillna(0).astype('float32')\n",
    "        self.y_test = y_test\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        n_cols = self.X_train.shape[1]\n",
    "        model.add(BatchNormalization(input_shape=(n_cols,)))\n",
    "        model.add(Dense(70, activation='linear'))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(50, activation='relu'))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(50, activation='relu'))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "        sgd = keras.optimizers.SGD(lr=.001, decay=2e-4, momentum=0.9, nesterov=True)\n",
    "        model.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def train(self, patience=50, epochs=200):\n",
    "        early_stopping_monitor = EarlyStopping(patience=patience)\n",
    "        history = self.model.fit(self.X_train, self.y_train,\n",
    "                                 validation_split=0.3, epochs=epochs, callbacks=[early_stopping_monitor])\n",
    "        return history\n",
    "\n",
    "    def evaluate(self):\n",
    "        score = self.model.evaluate(self.X_test, self.y_test, verbose=0)\n",
    "        return score\n",
    "\n",
    "    def fit(self):\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2566f858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/puddinpop/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/puddinpop/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/puddinpop/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "444/444 [==============================] - 4s 4ms/step - loss: 0.3175 - accuracy: 0.5102 - val_loss: 0.2644 - val_accuracy: 0.6023\n",
      "Epoch 2/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.2888 - accuracy: 0.4964 - val_loss: 0.2549 - val_accuracy: 0.5901\n",
      "Epoch 3/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.2682 - accuracy: 0.4890 - val_loss: 0.2514 - val_accuracy: 0.5694\n",
      "Epoch 4/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.2573 - accuracy: 0.4949 - val_loss: 0.2504 - val_accuracy: 0.5497\n",
      "Epoch 5/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.2526 - accuracy: 0.5017 - val_loss: 0.2501 - val_accuracy: 0.5339\n",
      "Epoch 6/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.2509 - accuracy: 0.4900 - val_loss: 0.2500 - val_accuracy: 0.5221\n",
      "Epoch 7/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.2504 - accuracy: 0.4959 - val_loss: 0.2500 - val_accuracy: 0.5190\n",
      "Epoch 8/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.2502 - accuracy: 0.4860 - val_loss: 0.2500 - val_accuracy: 0.4871\n",
      "Epoch 9/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.2501 - accuracy: 0.4807 - val_loss: 0.2500 - val_accuracy: 0.4380\n",
      "Epoch 10/200\n",
      "444/444 [==============================] - 2s 5ms/step - loss: 0.2500 - accuracy: 0.4729 - val_loss: 0.2500 - val_accuracy: 0.3918\n",
      "Epoch 11/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.2500 - accuracy: 0.4689 - val_loss: 0.2500 - val_accuracy: 0.3779\n",
      "Epoch 12/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.2500 - accuracy: 0.4633 - val_loss: 0.2500 - val_accuracy: 0.3595\n",
      "Epoch 13/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.2500 - accuracy: 0.4714 - val_loss: 0.2500 - val_accuracy: 0.3166\n",
      "Epoch 14/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.2500 - accuracy: 0.4533 - val_loss: 0.2500 - val_accuracy: 0.3259\n",
      "Epoch 15/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.2500 - accuracy: 0.4631 - val_loss: 0.2500 - val_accuracy: 0.3235\n",
      "Epoch 16/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.2500 - accuracy: 0.4567 - val_loss: 0.2500 - val_accuracy: 0.3172\n",
      "Epoch 17/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.2500 - accuracy: 0.4689 - val_loss: 0.2500 - val_accuracy: 0.3156\n",
      "Epoch 18/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.2500 - accuracy: 0.4607 - val_loss: 0.2500 - val_accuracy: 0.3153\n",
      "Epoch 19/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.2500 - accuracy: 0.4669 - val_loss: 0.2500 - val_accuracy: 0.3159\n",
      "Epoch 20/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.2500 - accuracy: 0.4665 - val_loss: 0.2500 - val_accuracy: 0.3309\n",
      "Epoch 21/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.2500 - accuracy: 0.4625 - val_loss: 0.2500 - val_accuracy: 0.3204\n",
      "Epoch 22/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.2500 - accuracy: 0.4638 - val_loss: 0.2500 - val_accuracy: 0.3192\n",
      "Epoch 23/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.2500 - accuracy: 0.4601 - val_loss: 0.2500 - val_accuracy: 0.3125\n",
      "Epoch 24/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.2500 - accuracy: 0.4625 - val_loss: 0.2500 - val_accuracy: 0.3146\n",
      "Epoch 25/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.2500 - accuracy: 0.4616 - val_loss: 0.2500 - val_accuracy: 0.3118\n",
      "Epoch 26/200\n",
      "444/444 [==============================] - 2s 5ms/step - loss: 0.2500 - accuracy: 0.4639 - val_loss: 0.2500 - val_accuracy: 0.3218\n",
      "Epoch 27/200\n",
      "444/444 [==============================] - 2s 5ms/step - loss: 0.2500 - accuracy: 0.4655 - val_loss: 0.2500 - val_accuracy: 0.3199\n",
      "Epoch 28/200\n",
      "444/444 [==============================] - 2s 5ms/step - loss: 0.2500 - accuracy: 0.4681 - val_loss: 0.2500 - val_accuracy: 0.3161\n",
      "Epoch 29/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.2500 - accuracy: 0.4710 - val_loss: 0.2500 - val_accuracy: 0.3110\n",
      "Epoch 30/200\n",
      "444/444 [==============================] - 2s 5ms/step - loss: 0.2500 - accuracy: 0.4674 - val_loss: 0.2500 - val_accuracy: 0.3253\n",
      "Epoch 31/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.2500 - accuracy: 0.4787 - val_loss: 0.2500 - val_accuracy: 0.3343\n",
      "Epoch 32/200\n",
      "444/444 [==============================] - 2s 5ms/step - loss: 0.2500 - accuracy: 0.4589 - val_loss: 0.2500 - val_accuracy: 0.3108\n",
      "Epoch 33/200\n",
      "444/444 [==============================] - 2s 5ms/step - loss: 0.2500 - accuracy: 0.4626 - val_loss: 0.2500 - val_accuracy: 0.3227\n",
      "Epoch 34/200\n",
      "444/444 [==============================] - 3s 6ms/step - loss: 0.2500 - accuracy: 0.4627 - val_loss: 0.2500 - val_accuracy: 0.3195\n",
      "Epoch 35/200\n",
      "444/444 [==============================] - 2s 5ms/step - loss: 0.2500 - accuracy: 0.4676 - val_loss: 0.2500 - val_accuracy: 0.3151\n",
      "Epoch 36/200\n",
      "444/444 [==============================] - 2s 5ms/step - loss: 0.2500 - accuracy: 0.4637 - val_loss: 0.2500 - val_accuracy: 0.3156\n",
      "Epoch 37/200\n",
      "444/444 [==============================] - 2s 5ms/step - loss: 0.2500 - accuracy: 0.4648 - val_loss: 0.2500 - val_accuracy: 0.3113\n",
      "Epoch 38/200\n",
      "444/444 [==============================] - 2s 5ms/step - loss: 0.2500 - accuracy: 0.4731 - val_loss: 0.2500 - val_accuracy: 0.3212\n",
      "Epoch 39/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.2500 - accuracy: 0.4739 - val_loss: 0.2500 - val_accuracy: 0.3144\n",
      "Epoch 40/200\n",
      "444/444 [==============================] - 3s 6ms/step - loss: 0.2500 - accuracy: 0.4614 - val_loss: 0.2500 - val_accuracy: 0.3164\n",
      "Epoch 41/200\n",
      "444/444 [==============================] - 2s 5ms/step - loss: 0.2500 - accuracy: 0.4579 - val_loss: 0.2500 - val_accuracy: 0.3144\n",
      "Epoch 42/200\n",
      "444/444 [==============================] - 2s 5ms/step - loss: 0.2500 - accuracy: 0.4538 - val_loss: 0.2500 - val_accuracy: 0.3153\n",
      "Epoch 43/200\n",
      "444/444 [==============================] - 2s 5ms/step - loss: 0.2500 - accuracy: 0.4581 - val_loss: 0.2500 - val_accuracy: 0.3098\n",
      "Epoch 44/200\n",
      "444/444 [==============================] - 2s 5ms/step - loss: 0.2500 - accuracy: 0.4645 - val_loss: 0.2500 - val_accuracy: 0.3190\n",
      "Epoch 45/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.2500 - accuracy: 0.4588 - val_loss: 0.2500 - val_accuracy: 0.3264\n",
      "Epoch 46/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.2500 - accuracy: 0.4660 - val_loss: 0.2500 - val_accuracy: 0.3243\n",
      "Epoch 47/200\n",
      "444/444 [==============================] - 2s 5ms/step - loss: 0.2500 - accuracy: 0.4680 - val_loss: 0.2500 - val_accuracy: 0.3177\n",
      "Epoch 48/200\n",
      "444/444 [==============================] - 2s 5ms/step - loss: 0.2500 - accuracy: 0.4556 - val_loss: 0.2500 - val_accuracy: 0.3143\n",
      "Epoch 49/200\n",
      "444/444 [==============================] - 2s 5ms/step - loss: 0.2500 - accuracy: 0.4652 - val_loss: 0.2500 - val_accuracy: 0.3212\n",
      "Epoch 50/200\n",
      "444/444 [==============================] - 2s 5ms/step - loss: 0.2500 - accuracy: 0.4689 - val_loss: 0.2500 - val_accuracy: 0.3384\n",
      "Epoch 51/200\n",
      "444/444 [==============================] - 2s 5ms/step - loss: 0.2500 - accuracy: 0.4644 - val_loss: 0.2500 - val_accuracy: 0.3427\n",
      "Epoch 52/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.2500 - accuracy: 0.4675 - val_loss: 0.2500 - val_accuracy: 0.3440\n",
      "Epoch 53/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.2500 - accuracy: 0.4843 - val_loss: 0.2500 - val_accuracy: 0.3641\n",
      "Epoch 54/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.2500 - accuracy: 0.4695 - val_loss: 0.2500 - val_accuracy: 0.3857\n",
      "Epoch 55/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.2500 - accuracy: 0.4896 - val_loss: 0.2500 - val_accuracy: 0.4112\n",
      "Epoch 56/200\n",
      "444/444 [==============================] - 2s 3ms/step - loss: 0.2500 - accuracy: 0.4884 - val_loss: 0.2500 - val_accuracy: 0.4449\n",
      "Epoch 57/200\n",
      "444/444 [==============================] - 2s 5ms/step - loss: 0.2500 - accuracy: 0.4930 - val_loss: 0.2500 - val_accuracy: 0.4761\n",
      "Epoch 58/200\n",
      "444/444 [==============================] - 2s 5ms/step - loss: 0.2500 - accuracy: 0.4885 - val_loss: 0.2500 - val_accuracy: 0.5200\n",
      "Epoch 59/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.2500 - accuracy: 0.4967 - val_loss: 0.2500 - val_accuracy: 0.5352\n",
      "Epoch 60/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.2500 - accuracy: 0.5000 - val_loss: 0.2500 - val_accuracy: 0.5370\n",
      "Epoch 61/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.2500 - accuracy: 0.5038 - val_loss: 0.2500 - val_accuracy: 0.5461\n",
      "Epoch 62/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.2500 - accuracy: 0.5040 - val_loss: 0.2500 - val_accuracy: 0.5599\n",
      "Epoch 63/200\n",
      "444/444 [==============================] - 2s 5ms/step - loss: 0.2500 - accuracy: 0.5043 - val_loss: 0.2500 - val_accuracy: 0.5637\n",
      "Epoch 64/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.2500 - accuracy: 0.5097 - val_loss: 0.2500 - val_accuracy: 0.5610\n",
      "Epoch 65/200\n",
      "444/444 [==============================] - 2s 3ms/step - loss: 0.2500 - accuracy: 0.5148 - val_loss: 0.2500 - val_accuracy: 0.5619\n",
      "Epoch 66/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.2500 - accuracy: 0.5093 - val_loss: 0.2500 - val_accuracy: 0.5755\n",
      "Epoch 67/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.2500 - accuracy: 0.5140 - val_loss: 0.2500 - val_accuracy: 0.5738\n",
      "Epoch 68/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.2500 - accuracy: 0.5081 - val_loss: 0.2500 - val_accuracy: 0.5745\n",
      "Epoch 69/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.2500 - accuracy: 0.5213 - val_loss: 0.2500 - val_accuracy: 0.5701\n",
      "Epoch 70/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.2500 - accuracy: 0.5108 - val_loss: 0.2500 - val_accuracy: 0.5732\n",
      "Epoch 71/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.2500 - accuracy: 0.5212 - val_loss: 0.2500 - val_accuracy: 0.5632\n",
      "634/634 [==============================] - 2s 3ms/step - loss: 0.2500 - accuracy: 0.5170\n",
      "159/159 [==============================] - 0s 1ms/step\n",
      "634/634 [==============================] - 1s 1ms/step\n",
      "159/159 [==============================] - 0s 1ms/step\n",
      "159/159 [==============================] - 0s 1ms/step\n",
      "MLP train Brier score: 0.2499998969793581 \n",
      " test Brier score: 0.24999989906528355 \n",
      " AUROC: 0.48568710253384817\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the data preprocessor and preprocess the data\n",
    "data_preprocessor = RecidivismDataPreprocessor(url)\n",
    "data_preprocessor.preprocess()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(\n",
    "    data_preprocessor.recidivism.drop(['ID', 'Recidivism_Within_3years', 'Recidivism_Arrest_Year1', 'Recidivism_Arrest_Year2', 'Recidivism_Arrest_Year3'], axis=1),\n",
    "    data_preprocessor.recidivism['Recidivism_Arrest_Year1'], test_size=0.2)\n",
    "\n",
    "# Instantiate the model evaluator\n",
    "model_evaluator = ModelEvaluator()\n",
    "\n",
    "# Instantiate and train the MLP model\n",
    "mlp_model = MLPModel(X_train1, y_train1, X_test1, y_test1)\n",
    "history = mlp_model.train()\n",
    "\n",
    "# Evaluate the MLP model\n",
    "score = mlp_model.evaluate()\n",
    "mlp_model.fit()\n",
    "\n",
    "# Calculate performance metrics\n",
    "fpr, tpr, _ = roc_curve(y_test1, model_evaluator.y_roc(mlp_model.model, mlp_model.X_test))\n",
    "train_brier_score = model_evaluator.brier_score(mlp_model.model.predict(mlp_model.X_train), y_train1)\n",
    "test_brier_score = model_evaluator.brier_score(mlp_model.model.predict(mlp_model.X_test), y_test1)\n",
    "auroc = roc_auc_score(y_test1, model_evaluator.y_roc(mlp_model.model, mlp_model.X_test))\n",
    "\n",
    "# Print results\n",
    "print('MLP train Brier score:', train_brier_score,\n",
    "      '\\n test Brier score:', test_brier_score,\n",
    "      '\\n AUROC:', auroc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b54fdf",
   "metadata": {},
   "source": [
    "#### Multi Layer Perceptron from Categorical Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3a10e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/puddinpop/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444/444 [==============================] - 2s 3ms/step - loss: 0.7025 - accuracy: 0.6340 - val_loss: 0.5665 - val_accuracy: 0.7031\n",
      "Epoch 2/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.6016 - accuracy: 0.6928 - val_loss: 0.5634 - val_accuracy: 0.7025\n",
      "Epoch 3/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5805 - accuracy: 0.7033 - val_loss: 0.5578 - val_accuracy: 0.7114\n",
      "Epoch 4/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5737 - accuracy: 0.7047 - val_loss: 0.5542 - val_accuracy: 0.7095\n",
      "Epoch 5/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5722 - accuracy: 0.7062 - val_loss: 0.5532 - val_accuracy: 0.7100\n",
      "Epoch 6/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5646 - accuracy: 0.7125 - val_loss: 0.5486 - val_accuracy: 0.7169\n",
      "Epoch 7/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5663 - accuracy: 0.7106 - val_loss: 0.5460 - val_accuracy: 0.7220\n",
      "Epoch 8/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5622 - accuracy: 0.7134 - val_loss: 0.5444 - val_accuracy: 0.7235\n",
      "Epoch 9/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5599 - accuracy: 0.7117 - val_loss: 0.5445 - val_accuracy: 0.7199\n",
      "Epoch 10/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5583 - accuracy: 0.7131 - val_loss: 0.5414 - val_accuracy: 0.7238\n",
      "Epoch 11/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5564 - accuracy: 0.7141 - val_loss: 0.5412 - val_accuracy: 0.7229\n",
      "Epoch 12/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5564 - accuracy: 0.7177 - val_loss: 0.5410 - val_accuracy: 0.7247\n",
      "Epoch 13/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5557 - accuracy: 0.7142 - val_loss: 0.5407 - val_accuracy: 0.7237\n",
      "Epoch 14/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5527 - accuracy: 0.7170 - val_loss: 0.5422 - val_accuracy: 0.7263\n",
      "Epoch 15/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5541 - accuracy: 0.7179 - val_loss: 0.5395 - val_accuracy: 0.7252\n",
      "Epoch 16/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5537 - accuracy: 0.7178 - val_loss: 0.5385 - val_accuracy: 0.7237\n",
      "Epoch 17/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5539 - accuracy: 0.7169 - val_loss: 0.5391 - val_accuracy: 0.7252\n",
      "Epoch 18/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5541 - accuracy: 0.7178 - val_loss: 0.5405 - val_accuracy: 0.7227\n",
      "Epoch 19/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5510 - accuracy: 0.7202 - val_loss: 0.5393 - val_accuracy: 0.7248\n",
      "Epoch 20/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5503 - accuracy: 0.7194 - val_loss: 0.5389 - val_accuracy: 0.7240\n",
      "Epoch 21/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5512 - accuracy: 0.7183 - val_loss: 0.5363 - val_accuracy: 0.7252\n",
      "Epoch 22/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5526 - accuracy: 0.7189 - val_loss: 0.5416 - val_accuracy: 0.7240\n",
      "Epoch 23/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5512 - accuracy: 0.7163 - val_loss: 0.5365 - val_accuracy: 0.7256\n",
      "Epoch 24/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5496 - accuracy: 0.7192 - val_loss: 0.5374 - val_accuracy: 0.7256\n",
      "Epoch 25/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5513 - accuracy: 0.7193 - val_loss: 0.5361 - val_accuracy: 0.7273\n",
      "Epoch 26/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5494 - accuracy: 0.7180 - val_loss: 0.5362 - val_accuracy: 0.7273\n",
      "Epoch 27/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5511 - accuracy: 0.7169 - val_loss: 0.5365 - val_accuracy: 0.7258\n",
      "Epoch 28/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5502 - accuracy: 0.7192 - val_loss: 0.5384 - val_accuracy: 0.7245\n",
      "Epoch 29/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5492 - accuracy: 0.7204 - val_loss: 0.5361 - val_accuracy: 0.7255\n",
      "Epoch 30/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5490 - accuracy: 0.7192 - val_loss: 0.5369 - val_accuracy: 0.7253\n",
      "Epoch 31/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5481 - accuracy: 0.7197 - val_loss: 0.5361 - val_accuracy: 0.7261\n",
      "Epoch 32/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5494 - accuracy: 0.7159 - val_loss: 0.5384 - val_accuracy: 0.7255\n",
      "Epoch 33/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5491 - accuracy: 0.7173 - val_loss: 0.5363 - val_accuracy: 0.7242\n",
      "Epoch 34/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5491 - accuracy: 0.7192 - val_loss: 0.5357 - val_accuracy: 0.7266\n",
      "Epoch 35/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5474 - accuracy: 0.7207 - val_loss: 0.5350 - val_accuracy: 0.7289\n",
      "Epoch 36/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5498 - accuracy: 0.7185 - val_loss: 0.5352 - val_accuracy: 0.7266\n",
      "Epoch 37/200\n",
      "444/444 [==============================] - 2s 3ms/step - loss: 0.5491 - accuracy: 0.7211 - val_loss: 0.5367 - val_accuracy: 0.7248\n",
      "Epoch 38/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.5493 - accuracy: 0.7202 - val_loss: 0.5348 - val_accuracy: 0.7256\n",
      "Epoch 39/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5461 - accuracy: 0.7237 - val_loss: 0.5350 - val_accuracy: 0.7275\n",
      "Epoch 40/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5487 - accuracy: 0.7216 - val_loss: 0.5356 - val_accuracy: 0.7270\n",
      "Epoch 41/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.5485 - accuracy: 0.7202 - val_loss: 0.5360 - val_accuracy: 0.7265\n",
      "Epoch 42/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5490 - accuracy: 0.7188 - val_loss: 0.5349 - val_accuracy: 0.7271\n",
      "Epoch 43/200\n",
      "444/444 [==============================] - 2s 3ms/step - loss: 0.5483 - accuracy: 0.7188 - val_loss: 0.5342 - val_accuracy: 0.7270\n",
      "Epoch 44/200\n",
      "444/444 [==============================] - 2s 3ms/step - loss: 0.5472 - accuracy: 0.7199 - val_loss: 0.5353 - val_accuracy: 0.7270\n",
      "Epoch 45/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5489 - accuracy: 0.7192 - val_loss: 0.5363 - val_accuracy: 0.7275\n",
      "Epoch 46/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5447 - accuracy: 0.7215 - val_loss: 0.5346 - val_accuracy: 0.7256\n",
      "Epoch 47/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5462 - accuracy: 0.7203 - val_loss: 0.5351 - val_accuracy: 0.7283\n",
      "Epoch 48/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5469 - accuracy: 0.7243 - val_loss: 0.5369 - val_accuracy: 0.7230\n",
      "Epoch 49/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.5455 - accuracy: 0.7223 - val_loss: 0.5345 - val_accuracy: 0.7261\n",
      "Epoch 50/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5459 - accuracy: 0.7230 - val_loss: 0.5365 - val_accuracy: 0.7258\n",
      "Epoch 51/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5459 - accuracy: 0.7228 - val_loss: 0.5363 - val_accuracy: 0.7266\n",
      "Epoch 52/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5450 - accuracy: 0.7224 - val_loss: 0.5351 - val_accuracy: 0.7279\n",
      "Epoch 53/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5457 - accuracy: 0.7226 - val_loss: 0.5393 - val_accuracy: 0.7209\n",
      "Epoch 54/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5458 - accuracy: 0.7229 - val_loss: 0.5376 - val_accuracy: 0.7256\n",
      "Epoch 55/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5480 - accuracy: 0.7212 - val_loss: 0.5353 - val_accuracy: 0.7270\n",
      "Epoch 56/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5481 - accuracy: 0.7212 - val_loss: 0.5364 - val_accuracy: 0.7270\n",
      "Epoch 57/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5453 - accuracy: 0.7210 - val_loss: 0.5340 - val_accuracy: 0.7299\n",
      "Epoch 58/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5473 - accuracy: 0.7204 - val_loss: 0.5342 - val_accuracy: 0.7275\n",
      "Epoch 59/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5472 - accuracy: 0.7253 - val_loss: 0.5350 - val_accuracy: 0.7273\n",
      "Epoch 60/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5460 - accuracy: 0.7238 - val_loss: 0.5381 - val_accuracy: 0.7215\n",
      "Epoch 61/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5458 - accuracy: 0.7237 - val_loss: 0.5338 - val_accuracy: 0.7279\n",
      "Epoch 62/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5445 - accuracy: 0.7214 - val_loss: 0.5339 - val_accuracy: 0.7278\n",
      "Epoch 63/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5432 - accuracy: 0.7246 - val_loss: 0.5341 - val_accuracy: 0.7283\n",
      "Epoch 64/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5455 - accuracy: 0.7219 - val_loss: 0.5350 - val_accuracy: 0.7273\n",
      "Epoch 65/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5444 - accuracy: 0.7223 - val_loss: 0.5344 - val_accuracy: 0.7273\n",
      "Epoch 66/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5457 - accuracy: 0.7215 - val_loss: 0.5354 - val_accuracy: 0.7263\n",
      "Epoch 67/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5455 - accuracy: 0.7214 - val_loss: 0.5344 - val_accuracy: 0.7281\n",
      "Epoch 68/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5458 - accuracy: 0.7204 - val_loss: 0.5339 - val_accuracy: 0.7289\n",
      "Epoch 69/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5416 - accuracy: 0.7259 - val_loss: 0.5332 - val_accuracy: 0.7275\n",
      "Epoch 70/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5470 - accuracy: 0.7177 - val_loss: 0.5361 - val_accuracy: 0.7284\n",
      "Epoch 71/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5448 - accuracy: 0.7254 - val_loss: 0.5357 - val_accuracy: 0.7270\n",
      "Epoch 72/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5439 - accuracy: 0.7253 - val_loss: 0.5332 - val_accuracy: 0.7284\n",
      "Epoch 73/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5415 - accuracy: 0.7242 - val_loss: 0.5333 - val_accuracy: 0.7288\n",
      "Epoch 74/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5474 - accuracy: 0.7193 - val_loss: 0.5339 - val_accuracy: 0.7270\n",
      "Epoch 75/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5460 - accuracy: 0.7180 - val_loss: 0.5338 - val_accuracy: 0.7288\n",
      "Epoch 76/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5463 - accuracy: 0.7216 - val_loss: 0.5335 - val_accuracy: 0.7286\n",
      "Epoch 77/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5443 - accuracy: 0.7265 - val_loss: 0.5340 - val_accuracy: 0.7284\n",
      "Epoch 78/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5443 - accuracy: 0.7251 - val_loss: 0.5340 - val_accuracy: 0.7289\n",
      "Epoch 79/200\n",
      "444/444 [==============================] - 2s 3ms/step - loss: 0.5442 - accuracy: 0.7225 - val_loss: 0.5331 - val_accuracy: 0.7293\n",
      "Epoch 80/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5419 - accuracy: 0.7298 - val_loss: 0.5359 - val_accuracy: 0.7279\n",
      "Epoch 81/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5414 - accuracy: 0.7247 - val_loss: 0.5355 - val_accuracy: 0.7273\n",
      "Epoch 82/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5438 - accuracy: 0.7256 - val_loss: 0.5335 - val_accuracy: 0.7266\n",
      "Epoch 83/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5456 - accuracy: 0.7202 - val_loss: 0.5343 - val_accuracy: 0.7276\n",
      "Epoch 84/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5440 - accuracy: 0.7281 - val_loss: 0.5346 - val_accuracy: 0.7279\n",
      "Epoch 85/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5439 - accuracy: 0.7261 - val_loss: 0.5363 - val_accuracy: 0.7281\n",
      "Epoch 86/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5443 - accuracy: 0.7218 - val_loss: 0.5360 - val_accuracy: 0.7260\n",
      "Epoch 87/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5438 - accuracy: 0.7252 - val_loss: 0.5332 - val_accuracy: 0.7299\n",
      "Epoch 88/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5428 - accuracy: 0.7268 - val_loss: 0.5341 - val_accuracy: 0.7294\n",
      "Epoch 89/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5448 - accuracy: 0.7253 - val_loss: 0.5347 - val_accuracy: 0.7279\n",
      "Epoch 90/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5451 - accuracy: 0.7223 - val_loss: 0.5331 - val_accuracy: 0.7288\n",
      "Epoch 91/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5448 - accuracy: 0.7221 - val_loss: 0.5334 - val_accuracy: 0.7294\n",
      "Epoch 92/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5423 - accuracy: 0.7236 - val_loss: 0.5335 - val_accuracy: 0.7294\n",
      "Epoch 93/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5449 - accuracy: 0.7238 - val_loss: 0.5355 - val_accuracy: 0.7296\n",
      "Epoch 94/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5439 - accuracy: 0.7204 - val_loss: 0.5337 - val_accuracy: 0.7293\n",
      "Epoch 95/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5451 - accuracy: 0.7217 - val_loss: 0.5342 - val_accuracy: 0.7294\n",
      "Epoch 96/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5429 - accuracy: 0.7267 - val_loss: 0.5336 - val_accuracy: 0.7283\n",
      "Epoch 97/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5427 - accuracy: 0.7235 - val_loss: 0.5346 - val_accuracy: 0.7284\n",
      "Epoch 98/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5431 - accuracy: 0.7209 - val_loss: 0.5335 - val_accuracy: 0.7286\n",
      "Epoch 99/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5393 - accuracy: 0.7272 - val_loss: 0.5332 - val_accuracy: 0.7291\n",
      "Epoch 100/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.5446 - accuracy: 0.7247 - val_loss: 0.5344 - val_accuracy: 0.7268\n",
      "Epoch 101/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5433 - accuracy: 0.7245 - val_loss: 0.5335 - val_accuracy: 0.7294\n",
      "Epoch 102/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5449 - accuracy: 0.7242 - val_loss: 0.5324 - val_accuracy: 0.7283\n",
      "Epoch 103/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5437 - accuracy: 0.7280 - val_loss: 0.5334 - val_accuracy: 0.7301\n",
      "Epoch 104/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5449 - accuracy: 0.7212 - val_loss: 0.5372 - val_accuracy: 0.7229\n",
      "Epoch 105/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5427 - accuracy: 0.7253 - val_loss: 0.5335 - val_accuracy: 0.7289\n",
      "Epoch 106/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5412 - accuracy: 0.7257 - val_loss: 0.5357 - val_accuracy: 0.7273\n",
      "Epoch 107/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5444 - accuracy: 0.7235 - val_loss: 0.5359 - val_accuracy: 0.7284\n",
      "Epoch 108/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5425 - accuracy: 0.7268 - val_loss: 0.5332 - val_accuracy: 0.7281\n",
      "Epoch 109/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5425 - accuracy: 0.7270 - val_loss: 0.5338 - val_accuracy: 0.7298\n",
      "Epoch 110/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5437 - accuracy: 0.7269 - val_loss: 0.5363 - val_accuracy: 0.7273\n",
      "Epoch 111/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5430 - accuracy: 0.7250 - val_loss: 0.5340 - val_accuracy: 0.7289\n",
      "Epoch 112/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5433 - accuracy: 0.7247 - val_loss: 0.5332 - val_accuracy: 0.7302\n",
      "Epoch 113/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.5411 - accuracy: 0.7264 - val_loss: 0.5336 - val_accuracy: 0.7293\n",
      "Epoch 114/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.5411 - accuracy: 0.7265 - val_loss: 0.5330 - val_accuracy: 0.7283\n",
      "Epoch 115/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5436 - accuracy: 0.7276 - val_loss: 0.5338 - val_accuracy: 0.7301\n",
      "Epoch 116/200\n",
      "444/444 [==============================] - 2s 3ms/step - loss: 0.5453 - accuracy: 0.7250 - val_loss: 0.5336 - val_accuracy: 0.7302\n",
      "Epoch 117/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5423 - accuracy: 0.7231 - val_loss: 0.5337 - val_accuracy: 0.7302\n",
      "Epoch 118/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5427 - accuracy: 0.7264 - val_loss: 0.5380 - val_accuracy: 0.7210\n",
      "Epoch 119/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5442 - accuracy: 0.7245 - val_loss: 0.5336 - val_accuracy: 0.7286\n",
      "Epoch 120/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5423 - accuracy: 0.7240 - val_loss: 0.5340 - val_accuracy: 0.7283\n",
      "Epoch 121/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5427 - accuracy: 0.7253 - val_loss: 0.5338 - val_accuracy: 0.7299\n",
      "Epoch 122/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5433 - accuracy: 0.7263 - val_loss: 0.5338 - val_accuracy: 0.7294\n",
      "Epoch 123/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5407 - accuracy: 0.7268 - val_loss: 0.5330 - val_accuracy: 0.7283\n",
      "Epoch 124/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5434 - accuracy: 0.7216 - val_loss: 0.5330 - val_accuracy: 0.7288\n",
      "Epoch 125/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.5430 - accuracy: 0.7252 - val_loss: 0.5340 - val_accuracy: 0.7291\n",
      "Epoch 126/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5408 - accuracy: 0.7265 - val_loss: 0.5358 - val_accuracy: 0.7281\n",
      "Epoch 127/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5432 - accuracy: 0.7247 - val_loss: 0.5323 - val_accuracy: 0.7291\n",
      "Epoch 128/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5396 - accuracy: 0.7269 - val_loss: 0.5337 - val_accuracy: 0.7278\n",
      "Epoch 129/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5403 - accuracy: 0.7270 - val_loss: 0.5331 - val_accuracy: 0.7296\n",
      "Epoch 130/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5429 - accuracy: 0.7252 - val_loss: 0.5333 - val_accuracy: 0.7298\n",
      "Epoch 131/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5442 - accuracy: 0.7269 - val_loss: 0.5346 - val_accuracy: 0.7283\n",
      "Epoch 132/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5436 - accuracy: 0.7248 - val_loss: 0.5336 - val_accuracy: 0.7302\n",
      "Epoch 133/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5410 - accuracy: 0.7268 - val_loss: 0.5326 - val_accuracy: 0.7304\n",
      "Epoch 134/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5427 - accuracy: 0.7274 - val_loss: 0.5330 - val_accuracy: 0.7299\n",
      "Epoch 135/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5423 - accuracy: 0.7263 - val_loss: 0.5339 - val_accuracy: 0.7311\n",
      "Epoch 136/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5403 - accuracy: 0.7290 - val_loss: 0.5327 - val_accuracy: 0.7288\n",
      "Epoch 137/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5419 - accuracy: 0.7256 - val_loss: 0.5326 - val_accuracy: 0.7296\n",
      "Epoch 138/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5430 - accuracy: 0.7270 - val_loss: 0.5335 - val_accuracy: 0.7302\n",
      "Epoch 139/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5402 - accuracy: 0.7259 - val_loss: 0.5340 - val_accuracy: 0.7289\n",
      "Epoch 140/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5420 - accuracy: 0.7257 - val_loss: 0.5336 - val_accuracy: 0.7291\n",
      "Epoch 141/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5430 - accuracy: 0.7256 - val_loss: 0.5331 - val_accuracy: 0.7299\n",
      "Epoch 142/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5430 - accuracy: 0.7255 - val_loss: 0.5328 - val_accuracy: 0.7304\n",
      "Epoch 143/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5419 - accuracy: 0.7274 - val_loss: 0.5324 - val_accuracy: 0.7288\n",
      "Epoch 144/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5424 - accuracy: 0.7254 - val_loss: 0.5337 - val_accuracy: 0.7294\n",
      "Epoch 145/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5416 - accuracy: 0.7264 - val_loss: 0.5329 - val_accuracy: 0.7314\n",
      "Epoch 146/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5416 - accuracy: 0.7231 - val_loss: 0.5339 - val_accuracy: 0.7301\n",
      "Epoch 147/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5408 - accuracy: 0.7251 - val_loss: 0.5328 - val_accuracy: 0.7299\n",
      "Epoch 148/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5398 - accuracy: 0.7311 - val_loss: 0.5328 - val_accuracy: 0.7284\n",
      "Epoch 149/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5421 - accuracy: 0.7271 - val_loss: 0.5333 - val_accuracy: 0.7293\n",
      "Epoch 150/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5439 - accuracy: 0.7228 - val_loss: 0.5368 - val_accuracy: 0.7263\n",
      "Epoch 151/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5429 - accuracy: 0.7240 - val_loss: 0.5343 - val_accuracy: 0.7294\n",
      "Epoch 152/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5426 - accuracy: 0.7286 - val_loss: 0.5324 - val_accuracy: 0.7309\n",
      "Epoch 153/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5422 - accuracy: 0.7245 - val_loss: 0.5338 - val_accuracy: 0.7293\n",
      "Epoch 154/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5424 - accuracy: 0.7261 - val_loss: 0.5335 - val_accuracy: 0.7309\n",
      "Epoch 155/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5402 - accuracy: 0.7267 - val_loss: 0.5329 - val_accuracy: 0.7299\n",
      "Epoch 156/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5426 - accuracy: 0.7274 - val_loss: 0.5336 - val_accuracy: 0.7301\n",
      "Epoch 157/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5411 - accuracy: 0.7253 - val_loss: 0.5322 - val_accuracy: 0.7307\n",
      "Epoch 158/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5397 - accuracy: 0.7290 - val_loss: 0.5325 - val_accuracy: 0.7302\n",
      "Epoch 159/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5429 - accuracy: 0.7270 - val_loss: 0.5328 - val_accuracy: 0.7312\n",
      "Epoch 160/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5409 - accuracy: 0.7285 - val_loss: 0.5340 - val_accuracy: 0.7288\n",
      "Epoch 161/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5431 - accuracy: 0.7288 - val_loss: 0.5338 - val_accuracy: 0.7286\n",
      "Epoch 162/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5413 - accuracy: 0.7263 - val_loss: 0.5327 - val_accuracy: 0.7314\n",
      "Epoch 163/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.5426 - accuracy: 0.7253 - val_loss: 0.5335 - val_accuracy: 0.7288\n",
      "Epoch 164/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5398 - accuracy: 0.7282 - val_loss: 0.5333 - val_accuracy: 0.7299\n",
      "Epoch 165/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5425 - accuracy: 0.7260 - val_loss: 0.5330 - val_accuracy: 0.7294\n",
      "Epoch 166/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5412 - accuracy: 0.7291 - val_loss: 0.5328 - val_accuracy: 0.7319\n",
      "Epoch 167/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5415 - accuracy: 0.7269 - val_loss: 0.5323 - val_accuracy: 0.7309\n",
      "Epoch 168/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5397 - accuracy: 0.7261 - val_loss: 0.5333 - val_accuracy: 0.7309\n",
      "Epoch 169/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5413 - accuracy: 0.7257 - val_loss: 0.5336 - val_accuracy: 0.7301\n",
      "Epoch 170/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5421 - accuracy: 0.7290 - val_loss: 0.5336 - val_accuracy: 0.7275\n",
      "Epoch 171/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5420 - accuracy: 0.7293 - val_loss: 0.5332 - val_accuracy: 0.7296\n",
      "Epoch 172/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5414 - accuracy: 0.7257 - val_loss: 0.5339 - val_accuracy: 0.7312\n",
      "Epoch 173/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5427 - accuracy: 0.7259 - val_loss: 0.5324 - val_accuracy: 0.7301\n",
      "Epoch 174/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5412 - accuracy: 0.7252 - val_loss: 0.5323 - val_accuracy: 0.7296\n",
      "Epoch 175/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5395 - accuracy: 0.7278 - val_loss: 0.5332 - val_accuracy: 0.7288\n",
      "Epoch 176/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5424 - accuracy: 0.7235 - val_loss: 0.5324 - val_accuracy: 0.7306\n",
      "Epoch 177/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5402 - accuracy: 0.7272 - val_loss: 0.5338 - val_accuracy: 0.7265\n",
      "Epoch 178/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5419 - accuracy: 0.7262 - val_loss: 0.5333 - val_accuracy: 0.7288\n",
      "Epoch 179/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5394 - accuracy: 0.7272 - val_loss: 0.5316 - val_accuracy: 0.7311\n",
      "Epoch 180/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5392 - accuracy: 0.7281 - val_loss: 0.5319 - val_accuracy: 0.7304\n",
      "Epoch 181/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5397 - accuracy: 0.7245 - val_loss: 0.5320 - val_accuracy: 0.7296\n",
      "Epoch 182/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5399 - accuracy: 0.7291 - val_loss: 0.5317 - val_accuracy: 0.7311\n",
      "Epoch 183/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5409 - accuracy: 0.7280 - val_loss: 0.5337 - val_accuracy: 0.7298\n",
      "Epoch 184/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5423 - accuracy: 0.7273 - val_loss: 0.5335 - val_accuracy: 0.7299\n",
      "Epoch 185/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5417 - accuracy: 0.7277 - val_loss: 0.5330 - val_accuracy: 0.7296\n",
      "Epoch 186/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5419 - accuracy: 0.7259 - val_loss: 0.5359 - val_accuracy: 0.7291\n",
      "Epoch 187/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5397 - accuracy: 0.7257 - val_loss: 0.5342 - val_accuracy: 0.7279\n",
      "Epoch 188/200\n",
      "444/444 [==============================] - 2s 3ms/step - loss: 0.5396 - accuracy: 0.7273 - val_loss: 0.5317 - val_accuracy: 0.7298\n",
      "Epoch 189/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5408 - accuracy: 0.7271 - val_loss: 0.5326 - val_accuracy: 0.7298\n",
      "Epoch 190/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5394 - accuracy: 0.7248 - val_loss: 0.5344 - val_accuracy: 0.7283\n",
      "Epoch 191/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5399 - accuracy: 0.7305 - val_loss: 0.5323 - val_accuracy: 0.7309\n",
      "Epoch 192/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5414 - accuracy: 0.7266 - val_loss: 0.5335 - val_accuracy: 0.7296\n",
      "Epoch 193/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5410 - accuracy: 0.7269 - val_loss: 0.5330 - val_accuracy: 0.7293\n",
      "Epoch 194/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5386 - accuracy: 0.7294 - val_loss: 0.5343 - val_accuracy: 0.7288\n",
      "Epoch 195/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5418 - accuracy: 0.7278 - val_loss: 0.5327 - val_accuracy: 0.7306\n",
      "Epoch 196/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5409 - accuracy: 0.7259 - val_loss: 0.5318 - val_accuracy: 0.7316\n",
      "Epoch 197/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5406 - accuracy: 0.7270 - val_loss: 0.5320 - val_accuracy: 0.7314\n",
      "Epoch 198/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5388 - accuracy: 0.7284 - val_loss: 0.5321 - val_accuracy: 0.7314\n",
      "Epoch 199/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5402 - accuracy: 0.7288 - val_loss: 0.5330 - val_accuracy: 0.7291\n",
      "Epoch 200/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5395 - accuracy: 0.7249 - val_loss: 0.5324 - val_accuracy: 0.7293\n",
      "634/634 [==============================] - 1s 2ms/step\n",
      "159/159 [==============================] - 0s 1ms/step\n",
      "159/159 [==============================] - 0s 1ms/step\n",
      "MLP train Brier score: 0.1768673161663764 \n",
      " test Brier score: 0.17605844579651975 \n",
      " AUROC: 0.7361056572780836\n"
     ]
    }
   ],
   "source": [
    "class MLPModel:\n",
    "    def __init__(self, input_shape, loss):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(BatchNormalization(input_shape=input_shape))\n",
    "        self.model.add(Dense(70, activation='linear'))\n",
    "        self.model.add(Dropout(0.3))\n",
    "        self.model.add(Dense(50, activation='relu'))\n",
    "        self.model.add(Dropout(0.3))\n",
    "        self.model.add(Dense(50, activation='relu'))\n",
    "        self.model.add(Dropout(0.3))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Dense(2, activation='softmax'))\n",
    "        self.evaluator = ModelEvaluator()\n",
    "\n",
    "        self.early_stopping_monitor = EarlyStopping(patience=50)\n",
    "        sgd = keras.optimizers.SGD(lr=.001, decay=2e-4, momentum=0.9, nesterov=True)\n",
    "        self.model.compile(loss=loss, optimizer='sgd', metrics=['accuracy'])\n",
    "        self.history = None\n",
    "\n",
    "    def fit(self, X, y, validation_split=0.3, epochs=200):\n",
    "        self.history = self.model.fit(X.fillna(0).astype('float32'), y,\n",
    "                                      validation_split=validation_split, epochs=epochs, callbacks=[self.early_stopping_monitor ])\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        return self.model.evaluate(X.fillna(0).astype('float32'), y, verbose=0)\n",
    "\n",
    "    def get_roc_curve(self, X, y):\n",
    "        return roc_curve(y, self.evaluator.y_roc(self.model, X.fillna(0).astype('float32')))\n",
    "\n",
    "    def get_brier_score(self, X, y):\n",
    "        return self.evaluator.brier_score(self.model.predict(X.fillna(0).astype('float32')), y)\n",
    "\n",
    "\n",
    "# Multi Layer Perceptron from Categorical Cross Entropy\n",
    "n_cols = X_train1.shape[1]\n",
    "input_shape = (n_cols,)\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "\n",
    "cce_perceptron = MLPModel(input_shape, loss)\n",
    "cce_perceptron.fit(X_train1, y_train1)\n",
    "score = cce_perceptron.evaluate(X_test1, y_test1)\n",
    "evaluator = ModelEvaluator()\n",
    "\n",
    "train_brier_score = cce_perceptron.get_brier_score(X_train1, y_train1)\n",
    "test_brier_score = cce_perceptron.get_brier_score(X_test1, y_test1)\n",
    "auroc = roc_auc_score(y_test1, evaluator.y_roc(cce_perceptron.model, X_test1.fillna(0).astype('float32')))\n",
    "\n",
    "print('MLP train Brier score:', train_brier_score,\n",
    "      '\\n test Brier score:', test_brier_score,\n",
    "      '\\n AUROC:', auroc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5295acd9",
   "metadata": {},
   "source": [
    "#### Multi Layer Perceptron from Categorical Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "697f3dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "444/444 [==============================] - 4s 7ms/step - loss: 0.6649 - accuracy: 0.6501 - val_loss: 0.5552 - val_accuracy: 0.7069\n",
      "Epoch 2/50\n",
      "444/444 [==============================] - 3s 7ms/step - loss: 0.5829 - accuracy: 0.6999 - val_loss: 0.5389 - val_accuracy: 0.7266\n",
      "Epoch 3/50\n",
      "444/444 [==============================] - 3s 7ms/step - loss: 0.5586 - accuracy: 0.7166 - val_loss: 0.5358 - val_accuracy: 0.7243\n",
      "Epoch 4/50\n",
      "444/444 [==============================] - 3s 7ms/step - loss: 0.5568 - accuracy: 0.7132 - val_loss: 0.5360 - val_accuracy: 0.7258\n",
      "Epoch 5/50\n",
      "444/444 [==============================] - 3s 7ms/step - loss: 0.5526 - accuracy: 0.7163 - val_loss: 0.5408 - val_accuracy: 0.7214\n",
      "Epoch 6/50\n",
      "444/444 [==============================] - 3s 7ms/step - loss: 0.5478 - accuracy: 0.7242 - val_loss: 0.5332 - val_accuracy: 0.7278\n",
      "Epoch 7/50\n",
      "444/444 [==============================] - 3s 7ms/step - loss: 0.5453 - accuracy: 0.7275 - val_loss: 0.5337 - val_accuracy: 0.7317\n",
      "Epoch 8/50\n",
      "444/444 [==============================] - 3s 7ms/step - loss: 0.5436 - accuracy: 0.7250 - val_loss: 0.5332 - val_accuracy: 0.7283\n",
      "Epoch 9/50\n",
      "444/444 [==============================] - 4s 9ms/step - loss: 0.5422 - accuracy: 0.7290 - val_loss: 0.5332 - val_accuracy: 0.7335\n",
      "Epoch 10/50\n",
      "444/444 [==============================] - 4s 9ms/step - loss: 0.5420 - accuracy: 0.7287 - val_loss: 0.5357 - val_accuracy: 0.7252\n",
      "Epoch 11/50\n",
      "444/444 [==============================] - 4s 8ms/step - loss: 0.5429 - accuracy: 0.7261 - val_loss: 0.5320 - val_accuracy: 0.7307\n",
      "Epoch 12/50\n",
      "444/444 [==============================] - 4s 9ms/step - loss: 0.5409 - accuracy: 0.7282 - val_loss: 0.5327 - val_accuracy: 0.7298\n",
      "Epoch 13/50\n",
      "444/444 [==============================] - 4s 9ms/step - loss: 0.5367 - accuracy: 0.7314 - val_loss: 0.5323 - val_accuracy: 0.7324\n",
      "Epoch 14/50\n",
      "444/444 [==============================] - 4s 10ms/step - loss: 0.5389 - accuracy: 0.7289 - val_loss: 0.5366 - val_accuracy: 0.7258\n",
      "Epoch 15/50\n",
      "444/444 [==============================] - 4s 9ms/step - loss: 0.5355 - accuracy: 0.7326 - val_loss: 0.5352 - val_accuracy: 0.7252\n",
      "Epoch 16/50\n",
      "444/444 [==============================] - 4s 9ms/step - loss: 0.5348 - accuracy: 0.7332 - val_loss: 0.5316 - val_accuracy: 0.7317\n",
      "Epoch 17/50\n",
      "444/444 [==============================] - 5s 10ms/step - loss: 0.5362 - accuracy: 0.7327 - val_loss: 0.5359 - val_accuracy: 0.7227\n",
      "Epoch 18/50\n",
      "444/444 [==============================] - 4s 9ms/step - loss: 0.5331 - accuracy: 0.7345 - val_loss: 0.5351 - val_accuracy: 0.7302\n",
      "Epoch 19/50\n",
      "444/444 [==============================] - 3s 7ms/step - loss: 0.5322 - accuracy: 0.7349 - val_loss: 0.5319 - val_accuracy: 0.7319\n",
      "Epoch 20/50\n",
      "444/444 [==============================] - 3s 7ms/step - loss: 0.5309 - accuracy: 0.7359 - val_loss: 0.5330 - val_accuracy: 0.7307\n",
      "Epoch 21/50\n",
      "444/444 [==============================] - 3s 7ms/step - loss: 0.5294 - accuracy: 0.7327 - val_loss: 0.5328 - val_accuracy: 0.7284\n",
      "Epoch 22/50\n",
      "444/444 [==============================] - 3s 7ms/step - loss: 0.5286 - accuracy: 0.7371 - val_loss: 0.5325 - val_accuracy: 0.7329\n",
      "Epoch 23/50\n",
      "444/444 [==============================] - 3s 7ms/step - loss: 0.5292 - accuracy: 0.7402 - val_loss: 0.5334 - val_accuracy: 0.7322\n",
      "Epoch 24/50\n",
      "444/444 [==============================] - 3s 7ms/step - loss: 0.5264 - accuracy: 0.7409 - val_loss: 0.5325 - val_accuracy: 0.7312\n",
      "Epoch 25/50\n",
      "444/444 [==============================] - 3s 7ms/step - loss: 0.5249 - accuracy: 0.7401 - val_loss: 0.5341 - val_accuracy: 0.7301\n",
      "Epoch 26/50\n",
      "444/444 [==============================] - 3s 7ms/step - loss: 0.5260 - accuracy: 0.7414 - val_loss: 0.5361 - val_accuracy: 0.7261\n",
      "Epoch 27/50\n",
      "444/444 [==============================] - 3s 8ms/step - loss: 0.5246 - accuracy: 0.7418 - val_loss: 0.5337 - val_accuracy: 0.7288\n",
      "Epoch 28/50\n",
      "444/444 [==============================] - 3s 7ms/step - loss: 0.5226 - accuracy: 0.7440 - val_loss: 0.5348 - val_accuracy: 0.7271\n",
      "Epoch 29/50\n",
      "444/444 [==============================] - 3s 7ms/step - loss: 0.5231 - accuracy: 0.7428 - val_loss: 0.5351 - val_accuracy: 0.7335\n",
      "Epoch 30/50\n",
      "444/444 [==============================] - 3s 7ms/step - loss: 0.5193 - accuracy: 0.7472 - val_loss: 0.5341 - val_accuracy: 0.7276\n",
      "Epoch 31/50\n",
      "444/444 [==============================] - 3s 7ms/step - loss: 0.5199 - accuracy: 0.7404 - val_loss: 0.5353 - val_accuracy: 0.7317\n",
      "Epoch 32/50\n",
      "444/444 [==============================] - 3s 7ms/step - loss: 0.5156 - accuracy: 0.7467 - val_loss: 0.5376 - val_accuracy: 0.7265\n",
      "Epoch 33/50\n",
      "444/444 [==============================] - 3s 7ms/step - loss: 0.5148 - accuracy: 0.7466 - val_loss: 0.5388 - val_accuracy: 0.7225\n",
      "Epoch 34/50\n",
      "444/444 [==============================] - 3s 7ms/step - loss: 0.5160 - accuracy: 0.7510 - val_loss: 0.5387 - val_accuracy: 0.7293\n",
      "Epoch 35/50\n",
      "444/444 [==============================] - 3s 7ms/step - loss: 0.5127 - accuracy: 0.7454 - val_loss: 0.5386 - val_accuracy: 0.7271\n",
      "Epoch 36/50\n",
      "444/444 [==============================] - 3s 7ms/step - loss: 0.5122 - accuracy: 0.7545 - val_loss: 0.5410 - val_accuracy: 0.7278\n",
      "Epoch 37/50\n",
      "444/444 [==============================] - 3s 8ms/step - loss: 0.5121 - accuracy: 0.7497 - val_loss: 0.5414 - val_accuracy: 0.7263\n",
      "Epoch 38/50\n",
      "444/444 [==============================] - 3s 8ms/step - loss: 0.5063 - accuracy: 0.7537 - val_loss: 0.5427 - val_accuracy: 0.7248\n",
      "Epoch 39/50\n",
      "444/444 [==============================] - 3s 8ms/step - loss: 0.5109 - accuracy: 0.7492 - val_loss: 0.5422 - val_accuracy: 0.7260\n",
      "Epoch 40/50\n",
      "444/444 [==============================] - 3s 7ms/step - loss: 0.5039 - accuracy: 0.7576 - val_loss: 0.5447 - val_accuracy: 0.7237\n",
      "Epoch 41/50\n",
      "444/444 [==============================] - 3s 7ms/step - loss: 0.5034 - accuracy: 0.7541 - val_loss: 0.5454 - val_accuracy: 0.7256\n",
      "Epoch 42/50\n",
      "444/444 [==============================] - 4s 8ms/step - loss: 0.5048 - accuracy: 0.7568 - val_loss: 0.5468 - val_accuracy: 0.7222\n",
      "Epoch 43/50\n",
      "444/444 [==============================] - 3s 8ms/step - loss: 0.5014 - accuracy: 0.7566 - val_loss: 0.5421 - val_accuracy: 0.7250\n",
      "Epoch 44/50\n",
      "444/444 [==============================] - 3s 7ms/step - loss: 0.4978 - accuracy: 0.7594 - val_loss: 0.5475 - val_accuracy: 0.7184\n",
      "Epoch 45/50\n",
      "444/444 [==============================] - 3s 7ms/step - loss: 0.4974 - accuracy: 0.7594 - val_loss: 0.5460 - val_accuracy: 0.7204\n",
      "Epoch 46/50\n",
      "444/444 [==============================] - 3s 8ms/step - loss: 0.4936 - accuracy: 0.7644 - val_loss: 0.5483 - val_accuracy: 0.7197\n",
      "Epoch 47/50\n",
      "444/444 [==============================] - 4s 9ms/step - loss: 0.4947 - accuracy: 0.7635 - val_loss: 0.5527 - val_accuracy: 0.7192\n",
      "Epoch 48/50\n",
      "444/444 [==============================] - 3s 8ms/step - loss: 0.4861 - accuracy: 0.7671 - val_loss: 0.5519 - val_accuracy: 0.7199\n",
      "Epoch 49/50\n",
      "444/444 [==============================] - 3s 8ms/step - loss: 0.4940 - accuracy: 0.7623 - val_loss: 0.5530 - val_accuracy: 0.7156\n",
      "Epoch 50/50\n",
      "444/444 [==============================] - 3s 8ms/step - loss: 0.4857 - accuracy: 0.7687 - val_loss: 0.5542 - val_accuracy: 0.7179\n",
      "634/634 [==============================] - 2s 2ms/step\n",
      "159/159 [==============================] - 0s 2ms/step\n",
      "159/159 [==============================] - 1s 3ms/step\n",
      "MLP train Brier score: 0.15113181953803714 \n",
      " test Brier score: 0.18214959261450775 \n",
      " AUROC: 0.7207095418370234\n",
      "159/159 [==============================] - 1s 3ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class MLP:\n",
    "    def __init__(self, X_train, y_train, X_test, y_test):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.evaluator = ModelEvaluator()\n",
    "\n",
    "    def create_model(self, loss_func='mean_squared_error', optimizer='sgd', dropout_rate=0.3):\n",
    "        model = Sequential()\n",
    "        n_cols = self.X_train.shape[1]\n",
    "        model.add(BatchNormalization(input_shape=(n_cols,)))\n",
    "        model.add(Dense(1024, activation='relu'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        model.add(Dense(500, activation='relu'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "        model.compile(loss=loss_func, optimizer=optimizer, metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def train_model(self, model, epochs=50, batch_size=32):\n",
    "        early_stopping_monitor = EarlyStopping(patience=50)\n",
    "        history = model.fit(self.X_train.fillna(0).astype('float32'), self.y_train, validation_split=0.3,\n",
    "                            epochs=epochs, batch_size=batch_size, callbacks=[early_stopping_monitor])\n",
    "        return history\n",
    "\n",
    "    def evaluate_model(self, model):\n",
    "        train_brier_score = self.evaluator.brier_score(model.predict(self.X_train.fillna(0).astype('float32')), self.y_train)\n",
    "        test_brier_score = self.evaluator.brier_score(model.predict(self.X_test.fillna(0).astype('float32')), self.y_test)\n",
    "        y_pred = model.predict(self.X_test.fillna(0).astype('float32'))\n",
    "        auroc = roc_auc_score(self.y_test, self.evaluator.get_prob_1(y_pred))\n",
    "\n",
    "        print('MLP train Brier score:', train_brier_score,\n",
    "              '\\n test Brier score:', test_brier_score,\n",
    "              '\\n AUROC:', auroc)\n",
    "\n",
    "        return train_brier_score, test_brier_score, auroc\n",
    "\n",
    "    def plot_roc_curve(self, model):\n",
    "        fpr, tpr, _ = roc_curve(self.y_test, self.evaluator.y_roc(model, self.X_test.fillna(0).astype('float32')))\n",
    "        plt.plot(fpr, tpr, lw=2, alpha=0.3)\n",
    "        plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='black')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curve')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "mlp = MLP(X_train1, y_train1, X_test1, y_test1)\n",
    "\n",
    "# Create and compile the MLP model with categorical cross-entropy loss\n",
    "cce_mlp = mlp.create_model(loss_func='sparse_categorical_crossentropy', optimizer='sgd', dropout_rate=0.3)\n",
    "\n",
    "# Train the model\n",
    "history = mlp.train_model(cce_mlp, epochs=50, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "train_brier_score, test_brier_score, auroc = mlp.evaluate_model(cce_mlp)\n",
    "\n",
    "# Plot the ROC curve\n",
    "mlp.plot_roc_curve(cce_mlp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af819b77",
   "metadata": {},
   "source": [
    "#### Multi Layer Perceptron from Categorical Cross Entropy and Optimizer Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d22fd2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/puddinpop/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444/444 [==============================] - 5s 6ms/step - loss: 0.6970 - accuracy: 0.6402 - val_loss: 0.5582 - val_accuracy: 0.7061\n",
      "Epoch 2/200\n",
      "444/444 [==============================] - 2s 3ms/step - loss: 0.5816 - accuracy: 0.7019 - val_loss: 0.5433 - val_accuracy: 0.7209\n",
      "Epoch 3/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5613 - accuracy: 0.7097 - val_loss: 0.5396 - val_accuracy: 0.7245\n",
      "Epoch 4/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.5565 - accuracy: 0.7152 - val_loss: 0.5387 - val_accuracy: 0.7256\n",
      "Epoch 5/200\n",
      "444/444 [==============================] - 2s 5ms/step - loss: 0.5515 - accuracy: 0.7170 - val_loss: 0.5377 - val_accuracy: 0.7255\n",
      "Epoch 6/200\n",
      "444/444 [==============================] - 3s 6ms/step - loss: 0.5493 - accuracy: 0.7205 - val_loss: 0.5349 - val_accuracy: 0.7235\n",
      "Epoch 7/200\n",
      "444/444 [==============================] - 3s 7ms/step - loss: 0.5478 - accuracy: 0.7207 - val_loss: 0.5374 - val_accuracy: 0.7238\n",
      "Epoch 8/200\n",
      "444/444 [==============================] - 2s 5ms/step - loss: 0.5493 - accuracy: 0.7178 - val_loss: 0.5345 - val_accuracy: 0.7293\n",
      "Epoch 9/200\n",
      "444/444 [==============================] - 2s 5ms/step - loss: 0.5481 - accuracy: 0.7230 - val_loss: 0.5366 - val_accuracy: 0.7281\n",
      "Epoch 10/200\n",
      "444/444 [==============================] - 2s 5ms/step - loss: 0.5461 - accuracy: 0.7211 - val_loss: 0.5345 - val_accuracy: 0.7286\n",
      "Epoch 11/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.5430 - accuracy: 0.7249 - val_loss: 0.5331 - val_accuracy: 0.7299\n",
      "Epoch 12/200\n",
      "444/444 [==============================] - 2s 5ms/step - loss: 0.5459 - accuracy: 0.7242 - val_loss: 0.5357 - val_accuracy: 0.7298\n",
      "Epoch 13/200\n",
      "444/444 [==============================] - 2s 5ms/step - loss: 0.5440 - accuracy: 0.7254 - val_loss: 0.5340 - val_accuracy: 0.7298\n",
      "Epoch 14/200\n",
      "444/444 [==============================] - 2s 5ms/step - loss: 0.5455 - accuracy: 0.7248 - val_loss: 0.5367 - val_accuracy: 0.7289\n",
      "Epoch 15/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.5451 - accuracy: 0.7238 - val_loss: 0.5325 - val_accuracy: 0.7299\n",
      "Epoch 16/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.5420 - accuracy: 0.7252 - val_loss: 0.5372 - val_accuracy: 0.7288\n",
      "Epoch 17/200\n",
      "444/444 [==============================] - 2s 5ms/step - loss: 0.5401 - accuracy: 0.7278 - val_loss: 0.5336 - val_accuracy: 0.7309\n",
      "Epoch 18/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.5405 - accuracy: 0.7274 - val_loss: 0.5329 - val_accuracy: 0.7329\n",
      "Epoch 19/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5408 - accuracy: 0.7230 - val_loss: 0.5327 - val_accuracy: 0.7324\n",
      "Epoch 20/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5411 - accuracy: 0.7257 - val_loss: 0.5329 - val_accuracy: 0.7316\n",
      "Epoch 21/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5403 - accuracy: 0.7269 - val_loss: 0.5319 - val_accuracy: 0.7299\n",
      "Epoch 22/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5398 - accuracy: 0.7266 - val_loss: 0.5342 - val_accuracy: 0.7301\n",
      "Epoch 23/200\n",
      "444/444 [==============================] - 2s 3ms/step - loss: 0.5407 - accuracy: 0.7277 - val_loss: 0.5323 - val_accuracy: 0.7311\n",
      "Epoch 24/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5403 - accuracy: 0.7268 - val_loss: 0.5343 - val_accuracy: 0.7281\n",
      "Epoch 25/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5390 - accuracy: 0.7295 - val_loss: 0.5333 - val_accuracy: 0.7279\n",
      "Epoch 26/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5391 - accuracy: 0.7276 - val_loss: 0.5323 - val_accuracy: 0.7321\n",
      "Epoch 27/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5380 - accuracy: 0.7318 - val_loss: 0.5326 - val_accuracy: 0.7279\n",
      "Epoch 28/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5398 - accuracy: 0.7261 - val_loss: 0.5318 - val_accuracy: 0.7314\n",
      "Epoch 29/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5384 - accuracy: 0.7270 - val_loss: 0.5327 - val_accuracy: 0.7302\n",
      "Epoch 30/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5386 - accuracy: 0.7270 - val_loss: 0.5315 - val_accuracy: 0.7301\n",
      "Epoch 31/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5371 - accuracy: 0.7269 - val_loss: 0.5348 - val_accuracy: 0.7309\n",
      "Epoch 32/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5372 - accuracy: 0.7332 - val_loss: 0.5327 - val_accuracy: 0.7302\n",
      "Epoch 33/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5360 - accuracy: 0.7290 - val_loss: 0.5320 - val_accuracy: 0.7299\n",
      "Epoch 34/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5376 - accuracy: 0.7299 - val_loss: 0.5309 - val_accuracy: 0.7307\n",
      "Epoch 35/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.5366 - accuracy: 0.7260 - val_loss: 0.5326 - val_accuracy: 0.7314\n",
      "Epoch 36/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5371 - accuracy: 0.7293 - val_loss: 0.5306 - val_accuracy: 0.7304\n",
      "Epoch 37/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5391 - accuracy: 0.7274 - val_loss: 0.5320 - val_accuracy: 0.7306\n",
      "Epoch 38/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5382 - accuracy: 0.7290 - val_loss: 0.5317 - val_accuracy: 0.7304\n",
      "Epoch 39/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5352 - accuracy: 0.7290 - val_loss: 0.5316 - val_accuracy: 0.7294\n",
      "Epoch 40/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5381 - accuracy: 0.7299 - val_loss: 0.5318 - val_accuracy: 0.7304\n",
      "Epoch 41/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5364 - accuracy: 0.7285 - val_loss: 0.5314 - val_accuracy: 0.7294\n",
      "Epoch 42/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5365 - accuracy: 0.7282 - val_loss: 0.5334 - val_accuracy: 0.7311\n",
      "Epoch 43/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5341 - accuracy: 0.7343 - val_loss: 0.5312 - val_accuracy: 0.7307\n",
      "Epoch 44/200\n",
      "444/444 [==============================] - 2s 5ms/step - loss: 0.5360 - accuracy: 0.7346 - val_loss: 0.5324 - val_accuracy: 0.7335\n",
      "Epoch 45/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.5376 - accuracy: 0.7254 - val_loss: 0.5327 - val_accuracy: 0.7291\n",
      "Epoch 46/200\n",
      "444/444 [==============================] - 3s 6ms/step - loss: 0.5346 - accuracy: 0.7316 - val_loss: 0.5316 - val_accuracy: 0.7312\n",
      "Epoch 47/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.5345 - accuracy: 0.7328 - val_loss: 0.5349 - val_accuracy: 0.7299\n",
      "Epoch 48/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.5358 - accuracy: 0.7297 - val_loss: 0.5315 - val_accuracy: 0.7311\n",
      "Epoch 49/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.5339 - accuracy: 0.7280 - val_loss: 0.5330 - val_accuracy: 0.7301\n",
      "Epoch 50/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.5329 - accuracy: 0.7326 - val_loss: 0.5332 - val_accuracy: 0.7284\n",
      "Epoch 51/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.5365 - accuracy: 0.7312 - val_loss: 0.5334 - val_accuracy: 0.7316\n",
      "Epoch 52/200\n",
      "444/444 [==============================] - 2s 3ms/step - loss: 0.5343 - accuracy: 0.7326 - val_loss: 0.5327 - val_accuracy: 0.7316\n",
      "Epoch 53/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5318 - accuracy: 0.7364 - val_loss: 0.5318 - val_accuracy: 0.7301\n",
      "Epoch 54/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5335 - accuracy: 0.7344 - val_loss: 0.5332 - val_accuracy: 0.7327\n",
      "Epoch 55/200\n",
      "444/444 [==============================] - 2s 5ms/step - loss: 0.5333 - accuracy: 0.7298 - val_loss: 0.5350 - val_accuracy: 0.7304\n",
      "Epoch 56/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.5357 - accuracy: 0.7306 - val_loss: 0.5339 - val_accuracy: 0.7294\n",
      "Epoch 57/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.5354 - accuracy: 0.7332 - val_loss: 0.5339 - val_accuracy: 0.7293\n",
      "Epoch 58/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.5354 - accuracy: 0.7299 - val_loss: 0.5316 - val_accuracy: 0.7283\n",
      "Epoch 59/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5339 - accuracy: 0.7301 - val_loss: 0.5326 - val_accuracy: 0.7301\n",
      "Epoch 60/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5346 - accuracy: 0.7339 - val_loss: 0.5320 - val_accuracy: 0.7299\n",
      "Epoch 61/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.5339 - accuracy: 0.7295 - val_loss: 0.5320 - val_accuracy: 0.7312\n",
      "Epoch 62/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.5337 - accuracy: 0.7330 - val_loss: 0.5340 - val_accuracy: 0.7296\n",
      "Epoch 63/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.5331 - accuracy: 0.7316 - val_loss: 0.5313 - val_accuracy: 0.7311\n",
      "Epoch 64/200\n",
      "444/444 [==============================] - 2s 5ms/step - loss: 0.5340 - accuracy: 0.7322 - val_loss: 0.5336 - val_accuracy: 0.7314\n",
      "Epoch 65/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.5347 - accuracy: 0.7289 - val_loss: 0.5342 - val_accuracy: 0.7302\n",
      "Epoch 66/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5304 - accuracy: 0.7318 - val_loss: 0.5331 - val_accuracy: 0.7311\n",
      "Epoch 67/200\n",
      "444/444 [==============================] - 3s 6ms/step - loss: 0.5313 - accuracy: 0.7340 - val_loss: 0.5347 - val_accuracy: 0.7304\n",
      "Epoch 68/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5347 - accuracy: 0.7323 - val_loss: 0.5334 - val_accuracy: 0.7317\n",
      "Epoch 69/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.5309 - accuracy: 0.7290 - val_loss: 0.5332 - val_accuracy: 0.7317\n",
      "Epoch 70/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.5332 - accuracy: 0.7322 - val_loss: 0.5353 - val_accuracy: 0.7302\n",
      "Epoch 71/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.5322 - accuracy: 0.7330 - val_loss: 0.5345 - val_accuracy: 0.7270\n",
      "Epoch 72/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.5319 - accuracy: 0.7349 - val_loss: 0.5328 - val_accuracy: 0.7293\n",
      "Epoch 73/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.5308 - accuracy: 0.7351 - val_loss: 0.5338 - val_accuracy: 0.7275\n",
      "Epoch 74/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5308 - accuracy: 0.7368 - val_loss: 0.5326 - val_accuracy: 0.7281\n",
      "Epoch 75/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.5325 - accuracy: 0.7327 - val_loss: 0.5314 - val_accuracy: 0.7316\n",
      "Epoch 76/200\n",
      "444/444 [==============================] - 3s 6ms/step - loss: 0.5316 - accuracy: 0.7333 - val_loss: 0.5350 - val_accuracy: 0.7306\n",
      "Epoch 77/200\n",
      "444/444 [==============================] - 3s 7ms/step - loss: 0.5329 - accuracy: 0.7326 - val_loss: 0.5323 - val_accuracy: 0.7288\n",
      "Epoch 78/200\n",
      "444/444 [==============================] - 3s 6ms/step - loss: 0.5302 - accuracy: 0.7340 - val_loss: 0.5326 - val_accuracy: 0.7304\n",
      "Epoch 79/200\n",
      "444/444 [==============================] - 3s 8ms/step - loss: 0.5325 - accuracy: 0.7338 - val_loss: 0.5314 - val_accuracy: 0.7325\n",
      "Epoch 80/200\n",
      "444/444 [==============================] - 3s 7ms/step - loss: 0.5310 - accuracy: 0.7355 - val_loss: 0.5333 - val_accuracy: 0.7298\n",
      "Epoch 81/200\n",
      "444/444 [==============================] - 2s 5ms/step - loss: 0.5315 - accuracy: 0.7333 - val_loss: 0.5340 - val_accuracy: 0.7299\n",
      "Epoch 82/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.5323 - accuracy: 0.7327 - val_loss: 0.5344 - val_accuracy: 0.7311\n",
      "Epoch 83/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.5298 - accuracy: 0.7357 - val_loss: 0.5364 - val_accuracy: 0.7273\n",
      "Epoch 84/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.5302 - accuracy: 0.7341 - val_loss: 0.5330 - val_accuracy: 0.7271\n",
      "Epoch 85/200\n",
      "444/444 [==============================] - 2s 5ms/step - loss: 0.5295 - accuracy: 0.7340 - val_loss: 0.5335 - val_accuracy: 0.7309\n",
      "Epoch 86/200\n",
      "444/444 [==============================] - 2s 3ms/step - loss: 0.5315 - accuracy: 0.7349 - val_loss: 0.5362 - val_accuracy: 0.7273\n",
      "634/634 [==============================] - 1s 2ms/step\n",
      "159/159 [==============================] - 0s 2ms/step\n",
      "159/159 [==============================] - 0s 1ms/step\n",
      "MLP train Brier score: 0.17388670034557852 \n",
      " test Brier score: 0.17652019951129175 \n",
      " AUROC: 0.7358651314139836\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.17388670034557852, 0.17652019951129175, 0.7358651314139836)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP:\n",
    "    def __init__(self, X_train, y_train, X_test, y_test):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.evaluator = ModelEvaluator()\n",
    "\n",
    "        self.model = Sequential()\n",
    "\n",
    "        n_cols = X_train.shape[1]\n",
    "\n",
    "        self.model.add(BatchNormalization(input_shape=(n_cols,)))\n",
    "        self.model.add(Dense(70, activation='linear'))\n",
    "        self.model.add(Dropout(0.3))\n",
    "        self.model.add(Dense(50, activation='relu'))\n",
    "        self.model.add(Dropout(0.3))\n",
    "        self.model.add(Dense(50, activation='relu'))\n",
    "        self.model.add(Dropout(0.3))\n",
    "\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "        self.early_stopping_monitor = EarlyStopping(patience=50)\n",
    "        adam = keras.optimizers.Adam(lr=.001, decay=2e-4)\n",
    "        self.model.compile(loss='sparse_categorical_crossentropy',\n",
    "                           optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    def train(self, epochs=200):\n",
    "        history = self.model.fit(self.X_train.fillna(0).astype('float32'), self.y_train,\n",
    "                                 validation_split=0.3, epochs=epochs,\n",
    "                                 callbacks=[self.early_stopping_monitor])\n",
    "\n",
    "    def evaluate(self):\n",
    "        train_brier_score = self.evaluator.brier_score(\n",
    "            self.model.predict(self.X_train.fillna(0).astype('float32')), self.y_train)\n",
    "        test_brier_score = self.evaluator.brier_score(\n",
    "            self.model.predict(self.X_test.fillna(0).astype('float32')), self.y_test)\n",
    "        auroc = roc_auc_score(\n",
    "            self.y_test, self.evaluator.y_roc(self.model, self.X_test.fillna(0).astype('float32')))\n",
    "\n",
    "        print('MLP train Brier score:', train_brier_score,\n",
    "              '\\n test Brier score:', test_brier_score,\n",
    "              '\\n AUROC:', auroc)\n",
    "\n",
    "        return train_brier_score, test_brier_score, auroc\n",
    "mlp = MLP(X_train1, y_train1, X_test1, y_test1)\n",
    "mlp.train()\n",
    "mlp.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c62549",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
