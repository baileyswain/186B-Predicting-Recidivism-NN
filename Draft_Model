{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa12fd98",
   "metadata": {},
   "source": [
    "## PSYCH 186B Final Project\n",
    "### Predicting Recidiism with Multi Layer Perceptrons\n",
    "Winter 2023 Bailey Swain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1337f2",
   "metadata": {},
   "source": [
    "### What is the problem?\n",
    "The United States has the highest total number of incarcerated people around the world. Recidivism is the tendency of a convicted criminal to reoffend. 44% of formerly incarcerated people return to prison within the same year of being released. As time progresses the statistics just get worse with 68% returning within 3 years, 79% after 6 years, and 83% after 9. With recidivism rates as alarming as these many cities across America grapple with how to ethically integrate those who have been released back into society without compromising public safety."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f94acda",
   "metadata": {},
   "source": [
    "### Why is it interesting?\n",
    "This is interesting because the United States Federal Justice System has been seeking a way to empirically assess the risk of recidivism since the early twentieth century. [1][2] At the time the process used a correlation between age and criminal history to predict recidivism. By the 1970s these were extremely popular. Today the tools used to assess risk are able to use a large set of data including things like substance abuse, and employment. \n",
    "\n",
    "These tools are incredibly powerful because they are used to make very high stake decisions like whether someone will be granted bail, confinement form, community supervision conditions and revocation, being released between the time of arrest and trial. Machine learning is already widely used in the criminal justice system by using historical data to find patterns and form predictive accuracy. Something that makes it so interesting to me is the possibilities when it comes to discorverying new ways to make this type of algorithm impartial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe91025",
   "metadata": {},
   "source": [
    "### What has been done in literature?\n",
    "In the last 5 years there has been great efforts to advance recidivism prediction using machine learning. Northpointeâ€™s tool, called COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) has gained a lot of attention because it is a widely used commercial tool. However, there have been many data scientists who say the algorithm is racially biased against black offenders. \n",
    "\n",
    "Sarah Desmarais and Jay Singh studied almost 20 varying different recidivism risk tools in 2013. They found that most of the time these Machine Learning Algorithims being used had never been tested for validity. \n",
    "\n",
    "Since then the largest exploration of racial bias in the US recidivism Machine Learning algorithms was done at the University of California, Berkeley. In 2016 they reviewed 34,000 federal offenders and tested the predictive validity of a tool that was used by the federal courts to aid in probation and parole officers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54bae1e",
   "metadata": {},
   "source": [
    "### Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "80a6d530",
   "metadata": {},
   "outputs": [],
   "source": [
    "#arrays \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# plots\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ML\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression,Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV,train_test_split,cross_val_score,cross_validate\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score,roc_curve, auc, brier_score_loss\n",
    "from sklearn import tree\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pydotplus\n",
    "import graphviz\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from keras.layers import Dense, Input, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Activation, Embedding\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "#NN\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc09a1b9",
   "metadata": {},
   "source": [
    "My goal is to predict if a formerly incarcerated person will go bck to prison within 3 years based on their data. Knowing this is important because it will allow us to identify high risk individuals such that we can allocate more resouces to them or take time to focus on their education/habits. In order to evaluate if the model used is worth the resources, we can look at how well it stacks up against other prediction models and find ways to best tune it assuming we already want to find a way to identify high risk individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c2bbd68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "def y_roc(estimator,X):\n",
    "    y_scores=[]\n",
    "    for list in estimator.predict(X):\n",
    "        y_scores.append(list[1])\n",
    "    return y_scores\n",
    "def y_roc_regression(estimator,X):\n",
    "    y_scores=[]\n",
    "    for list in estimator.predict(X):\n",
    "        y_scores.append(list)\n",
    "    return y_scores\n",
    "def cv_roc_plot(estimator,X,y):\n",
    "    cv = StratifiedKFold(n_splits=4,shuffle=False)\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0,1,100)\n",
    "    for train,test in cv.split(X,y):\n",
    "        prediction = estimator.fit(X.iloc[train],y.iloc[train]).predict(X.iloc[test])\n",
    "        fpr, tpr, t = roc_curve(y.iloc[test], prediction[:, 1])\n",
    "        tpr[0]=0\n",
    "        tpr[-1]=1\n",
    "        tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        aucs.append(roc_auc)\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    return mean_fpr, mean_tpr, mean_auc\n",
    "def brier_score(y_prob_raw,y_true):\n",
    "    y_prob=[prob[1] for prob in y_prob_raw]\n",
    "    if len(y_prob)!=len(y_true):\n",
    "        print('Error: two lists must have same length')\n",
    "        return\n",
    "    out = 0\n",
    "    for prob_1,y in zip(y_prob,y_true):\n",
    "        out+=(prob_1-y)**2\n",
    "    return out/len(y_prob)\n",
    "def get_prob_1(y_prob_raw):\n",
    "    return [prob[1] for prob in y_prob_raw]\n",
    "def aver_prob(prob_lists):\n",
    "    n=len(prob_lists)\n",
    "    return np.sum(np.array(prob_lists),0)/n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80fd483",
   "metadata": {},
   "source": [
    "### Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bb1424a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the dataset\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/bswain7/MLP_Recidivism/nij_data/NIJ_s_Recidivism_Challenge_Full_Dataset.csv'\n",
    "recidivism = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0d06ab",
   "metadata": {},
   "source": [
    "### Initial Data Processing\n",
    "##### Changing and data to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "502639bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Gender', 'Race', 'Age_at_Release', 'Residence_PUMA',\n",
       "       'Gang_Affiliated', 'Supervision_Risk_Score_First',\n",
       "       'Supervision_Level_First', 'Education_Level', 'Dependents',\n",
       "       'Prison_Offense', 'Prison_Years', 'Prior_Arrest_Episodes_Felony',\n",
       "       'Prior_Arrest_Episodes_Misd', 'Prior_Arrest_Episodes_Violent',\n",
       "       'Prior_Arrest_Episodes_Property', 'Prior_Arrest_Episodes_Drug',\n",
       "       'Prior_Arrest_Episodes_PPViolationCharges',\n",
       "       'Prior_Arrest_Episodes_DVCharges', 'Prior_Arrest_Episodes_GunCharges',\n",
       "       'Prior_Conviction_Episodes_Felony', 'Prior_Conviction_Episodes_Misd',\n",
       "       'Prior_Conviction_Episodes_Viol', 'Prior_Conviction_Episodes_Prop',\n",
       "       'Prior_Conviction_Episodes_Drug',\n",
       "       'Prior_Conviction_Episodes_PPViolationCharges',\n",
       "       'Prior_Conviction_Episodes_DomesticViolenceCharges',\n",
       "       'Prior_Conviction_Episodes_GunCharges', 'Prior_Revocations_Parole',\n",
       "       'Prior_Revocations_Probation', 'Condition_MH_SA', 'Condition_Cog_Ed',\n",
       "       'Condition_Other', 'Delinquency_Reports', 'Program_Attendances',\n",
       "       'Program_UnexcusedAbsences', 'Residence_Changes',\n",
       "       'Recidivism_Within_3years', 'Recidivism_Arrest_Year1',\n",
       "       'Recidivism_Arrest_Year2', 'Recidivism_Arrest_Year3',\n",
       "       'Training_Sample'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop unnecessary columns\n",
    "cols_2_drop =['Violations_ElectronicMonitoring',\n",
    "       'Violations_Instruction', 'Violations_FailToReport',\n",
    "       'Violations_MoveWithoutPermission',\n",
    "       'Avg_Days_per_DrugTest', 'DrugTests_THC_Positive',\n",
    "       'DrugTests_Cocaine_Positive', 'DrugTests_Meth_Positive',\n",
    "       'DrugTests_Other_Positive', 'Percent_Days_Employed', 'Jobs_Per_Year',\n",
    "       'Employment_Exempt']\n",
    "\n",
    "recidivism = recidivism.drop(cols_2_drop, axis=1)\n",
    "recidivism.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "364f2525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the dtype of the feature from object to integer\n",
    "recidivism['Residence_PUMA']=recidivism['Residence_PUMA'].astype('category')\n",
    "recidivism['Residence_PUMA']=recidivism['Residence_PUMA'].cat.codes\n",
    "recidivism['Age_at_Release']=recidivism['Age_at_Release'].apply(lambda x: int(x[:2]))\n",
    "recidivism['Dependents']=recidivism['Dependents'].apply(lambda x: int(x[:1]))\n",
    "recidivism['Prior_Arrest_Episodes_Felony']=recidivism['Prior_Arrest_Episodes_Felony'].apply(lambda x: int(x[:2]))\n",
    "recidivism['Prior_Arrest_Episodes_Drug']=recidivism['Prior_Arrest_Episodes_Drug'].apply(lambda x: int(x[:2]))\n",
    "recidivism['Prior_Arrest_Episodes_Misd']=recidivism['Prior_Arrest_Episodes_Misd'].apply(lambda x: int(x[:2]))\n",
    "recidivism['Prior_Arrest_Episodes_Violent']=recidivism['Prior_Arrest_Episodes_Violent'].apply(lambda x: int(x[:2]))\n",
    "recidivism['Prior_Arrest_Episodes_Property']=recidivism['Prior_Arrest_Episodes_Property'].apply(lambda x: int(x[:2]))\n",
    "recidivism['Prior_Arrest_Episodes_PPViolationCharges']=recidivism['Prior_Arrest_Episodes_PPViolationCharges'].apply(lambda x: int(x[:2]))\n",
    "recidivism['Prior_Conviction_Episodes_Felony']=recidivism['Prior_Conviction_Episodes_Felony'].apply(lambda x: int(x[:1]))\n",
    "recidivism['Prior_Conviction_Episodes_Misd']=recidivism['Prior_Conviction_Episodes_Misd'].apply(lambda x: int(x[:1]))\n",
    "recidivism['Prior_Conviction_Episodes_Prop']=recidivism['Prior_Conviction_Episodes_Prop'].apply(lambda x: int(x[:1]))\n",
    "recidivism['Prior_Conviction_Episodes_Drug']=recidivism['Prior_Conviction_Episodes_Drug'].apply(lambda x: int(x[:1]))\n",
    "recidivism['Delinquency_Reports']=recidivism['Delinquency_Reports'].apply(lambda x: int(x[:1]))\n",
    "recidivism['Program_Attendances']=recidivism['Program_Attendances'].apply(lambda x: int(x[:2]))\n",
    "recidivism['Program_UnexcusedAbsences']=recidivism['Program_UnexcusedAbsences'].apply(lambda x: int(x[:1]))\n",
    "recidivism['Residence_Changes']=recidivism['Residence_Changes'].apply(lambda x: int(x[:1]))\n",
    "\n",
    "    # scale the columns which are not bool or category\n",
    "      \n",
    "scaler = StandardScaler()\n",
    "scaling_set=[]\n",
    "for column in recidivism.columns:\n",
    "    if recidivism[column].dtype == object:\n",
    "        recidivism[column]=recidivism[column].astype('category')\n",
    "        recidivism[column]=recidivism[column].cat.codes\n",
    "    elif recidivism[column].dtype in ['int64','float32','float64'] :\n",
    "        scaling_set+=[column]\n",
    "recidivism[scaling_set]=scaler.fit_transform(recidivism[scaling_set].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d81885ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/puddinpop/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/puddinpop/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# remove outliers\n",
    "recidivism=recidivism.drop(index=recidivism[recidivism.Supervision_Risk_Score_First.isnull()].index)\n",
    "recidivism=recidivism.drop(index=set(recidivism[recidivism.Supervision_Risk_Score_First.isnull()].index) & \n",
    "                         set(recidivism[recidivism.Prison_Offense .isnull()].index))\n",
    "recidivism=recidivism.reset_index(drop=True)\n",
    " \n",
    "    \n",
    "# impute missing value 'Supervision_Level_First' and 'Prison_Offense' with relative feature\n",
    "\n",
    "for missing_column in ['Supervision_Level_First','Prison_Offense']:\n",
    "    test_index=recidivism[recidivism[missing_column]==-1].index\n",
    "    train_index=recidivism[recidivism[missing_column]!=-1].index\n",
    "    X=recidivism.drop(columns=[missing_column])\n",
    "    y=recidivism[missing_column]\n",
    "    \n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(X.iloc[train_index,:],y[train_index])\n",
    "    recidivism.loc[test_index,missing_column]=logreg.predict(X.iloc[test_index,:])\n",
    "    recidivism[missing_column]=recidivism[missing_column].astype('category')\n",
    "    recidivism[missing_column]=recidivism[missing_column].cat.codes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18ea322",
   "metadata": {},
   "source": [
    "##### This model can act as a baseline for recidivism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aacbf825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split features and recidivism\n",
    "\n",
    "\n",
    "features = recidivism.drop(columns=[\"Recidivism_Within_3years\"])\n",
    "target = recidivism.Recidivism_Within_3years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "08f3efa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "  features, target, test_size=0.33, random_state=42)\n",
    "\n",
    "# convert to tensor arrays\n",
    "X_train_np = X_train.values\n",
    "X_test_np = X_test.values\n",
    "y_train_np = y_train.values\n",
    "y_test_np = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "03914eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(recidivism):\n",
    "  def __init__(self, X_train, y_train):\n",
    "    # need to convert float64 to float32 else \n",
    "    # will get the following error\n",
    "    # RuntimeError: expected scalar type Double but found Float\n",
    "    self.X = torch.from_numpy(X_train.astype(np.float32))\n",
    "    # need to convert float64 to Long else \n",
    "    # will get the following error\n",
    "    # RuntimeError: expected scalar type Long but found Float\n",
    "    self.y = torch.from_numpy(y_train).type(torch.LongTensor)\n",
    "    self.len = self.X.shape[0]\n",
    "  \n",
    "  def __getitem__(self, index):\n",
    "    return self.X[index], self.y[index]\n",
    "  def __len__(self):\n",
    "    return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2e5dc06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "traindata = Data(X_train_np,y_train_np)\n",
    "trainloader = DataLoader(traindata, batch_size=batch_size, \n",
    "                         shuffle=True, num_workers=0) # split even smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9957177c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network class parameters\n",
    "input_dim = len(features.columns)\n",
    "output_dim = 3\n",
    "hidden = (input_dim + output_dim) // # bt input and output dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e80df883",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.leakyrelu = nn.LeakyReLU(0.001)\n",
    "    self.linear1 = nn.Linear(input_dim,hidden)\n",
    "    self.linear2 = nn.Linear(hidden,output_dim)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.leakyrelu(self.linear1(x))\n",
    "    x = self.linear2(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "26606af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of Network(\n",
      "  (leakyrelu): LeakyReLU(negative_slope=0.001)\n",
      "  (linear1): Linear(in_features=41, out_features=22, bias=True)\n",
      "  (linear2): Linear(in_features=22, out_features=3, bias=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "clf = Network()\n",
    "print(clf.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bc9e0455",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() # best for categories\n",
    "optimizer = torch.optim.SGD(clf.parameters(), lr=0.001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4fa80e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix for evaluation - will be used later on\n",
    "def plot_confusion_matrix(matrix,title=\"\"): \n",
    "    #put the heatmap into the figure \n",
    "    sns.heatmap(data=matrix, annot=True, cmap=\"spring\")\n",
    "    status=[\"0\",\"1\"]\n",
    "    axis_ticks=np.arange(len(status))+0.4\n",
    "    \n",
    "    #sets x axis ticks to species names\n",
    "    plt.xticks(axis_ticks,status) \n",
    "    \n",
    "    #sets y axis ticks to species names \n",
    "    plt.yticks(axis_ticks,status) \n",
    "    plt.title(title)\n",
    "    plt.ylabel(\"Actual Label\")\n",
    "    plt.xlabel(\"Predicted Label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0a39c1",
   "metadata": {},
   "source": [
    "### Multi Layer Perceptron with Mean Sqaured Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a34263b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/puddinpop/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "444/444 [==============================] - 3s 4ms/step - loss: 0.3160 - accuracy: 0.5057 - val_loss: 0.2606 - val_accuracy: 0.5278\n",
      "Epoch 2/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2912 - accuracy: 0.4941 - val_loss: 0.2544 - val_accuracy: 0.5190\n",
      "Epoch 3/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2724 - accuracy: 0.4965 - val_loss: 0.2517 - val_accuracy: 0.5389\n",
      "Epoch 4/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2600 - accuracy: 0.5002 - val_loss: 0.2506 - val_accuracy: 0.5288\n",
      "Epoch 5/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2539 - accuracy: 0.5021 - val_loss: 0.2502 - val_accuracy: 0.5117\n",
      "Epoch 6/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2514 - accuracy: 0.4958 - val_loss: 0.2501 - val_accuracy: 0.5278\n",
      "Epoch 7/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2505 - accuracy: 0.5021 - val_loss: 0.2500 - val_accuracy: 0.5182\n",
      "Epoch 8/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2502 - accuracy: 0.4954 - val_loss: 0.2500 - val_accuracy: 0.5188\n",
      "Epoch 9/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2501 - accuracy: 0.5012 - val_loss: 0.2500 - val_accuracy: 0.5108\n",
      "Epoch 10/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.5109 - val_loss: 0.2500 - val_accuracy: 0.5140\n",
      "Epoch 11/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.4971 - val_loss: 0.2500 - val_accuracy: 0.5277\n",
      "Epoch 12/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.4974 - val_loss: 0.2500 - val_accuracy: 0.5198\n",
      "Epoch 13/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.5062 - val_loss: 0.2500 - val_accuracy: 0.5191\n",
      "Epoch 14/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.4989 - val_loss: 0.2500 - val_accuracy: 0.5104\n",
      "Epoch 15/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.5031 - val_loss: 0.2500 - val_accuracy: 0.5062\n",
      "Epoch 16/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.5033 - val_loss: 0.2500 - val_accuracy: 0.5255\n",
      "Epoch 17/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.4985 - val_loss: 0.2500 - val_accuracy: 0.5213\n",
      "Epoch 18/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.5040 - val_loss: 0.2500 - val_accuracy: 0.5236\n",
      "Epoch 19/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.4902 - val_loss: 0.2500 - val_accuracy: 0.5075\n",
      "Epoch 20/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.5010 - val_loss: 0.2500 - val_accuracy: 0.5246\n",
      "Epoch 21/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.5037 - val_loss: 0.2500 - val_accuracy: 0.5293\n",
      "Epoch 22/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.5012 - val_loss: 0.2500 - val_accuracy: 0.5119\n",
      "Epoch 23/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.5025 - val_loss: 0.2500 - val_accuracy: 0.4953\n",
      "Epoch 24/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.5031 - val_loss: 0.2500 - val_accuracy: 0.4922\n",
      "Epoch 25/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.4985 - val_loss: 0.2500 - val_accuracy: 0.4868\n",
      "Epoch 26/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.4981 - val_loss: 0.2500 - val_accuracy: 0.4748\n",
      "Epoch 27/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.4977 - val_loss: 0.2500 - val_accuracy: 0.4961\n",
      "Epoch 28/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.5060 - val_loss: 0.2500 - val_accuracy: 0.4935\n",
      "Epoch 29/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.4958 - val_loss: 0.2500 - val_accuracy: 0.4965\n",
      "Epoch 30/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.4986 - val_loss: 0.2500 - val_accuracy: 0.5086\n",
      "Epoch 31/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.4987 - val_loss: 0.2500 - val_accuracy: 0.5048\n",
      "Epoch 32/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.4983 - val_loss: 0.2500 - val_accuracy: 0.5057\n",
      "Epoch 33/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.4907 - val_loss: 0.2500 - val_accuracy: 0.5081\n",
      "Epoch 34/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.2500 - accuracy: 0.4983 - val_loss: 0.2500 - val_accuracy: 0.5104\n",
      "Epoch 35/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.2500 - accuracy: 0.4985 - val_loss: 0.2500 - val_accuracy: 0.5053\n",
      "Epoch 36/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.4986 - val_loss: 0.2500 - val_accuracy: 0.4948\n",
      "Epoch 37/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.4979 - val_loss: 0.2500 - val_accuracy: 0.4979\n",
      "Epoch 38/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.2500 - accuracy: 0.4979 - val_loss: 0.2500 - val_accuracy: 0.5103\n",
      "Epoch 39/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.2500 - accuracy: 0.4958 - val_loss: 0.2500 - val_accuracy: 0.5062\n",
      "Epoch 40/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.2500 - accuracy: 0.4988 - val_loss: 0.2500 - val_accuracy: 0.5044\n",
      "Epoch 41/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.5022 - val_loss: 0.2500 - val_accuracy: 0.4940\n",
      "Epoch 42/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.4977 - val_loss: 0.2500 - val_accuracy: 0.5001\n",
      "Epoch 43/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.2500 - accuracy: 0.4990 - val_loss: 0.2500 - val_accuracy: 0.5119\n",
      "Epoch 44/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.4924 - val_loss: 0.2500 - val_accuracy: 0.4942\n",
      "Epoch 45/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.4920 - val_loss: 0.2500 - val_accuracy: 0.4919\n",
      "Epoch 46/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.2500 - accuracy: 0.4989 - val_loss: 0.2500 - val_accuracy: 0.4961\n",
      "Epoch 47/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.2500 - accuracy: 0.4942 - val_loss: 0.2500 - val_accuracy: 0.4961\n",
      "Epoch 48/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.2500 - accuracy: 0.4966 - val_loss: 0.2500 - val_accuracy: 0.4851\n",
      "Epoch 49/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.2500 - accuracy: 0.4933 - val_loss: 0.2500 - val_accuracy: 0.4906\n",
      "Epoch 50/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.4964 - val_loss: 0.2500 - val_accuracy: 0.4901\n",
      "Epoch 51/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.4951 - val_loss: 0.2500 - val_accuracy: 0.4914\n",
      "Epoch 52/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.2500 - accuracy: 0.4982 - val_loss: 0.2500 - val_accuracy: 0.4930\n",
      "Epoch 53/200\n",
      "444/444 [==============================] - 2s 5ms/step - loss: 0.2500 - accuracy: 0.4996 - val_loss: 0.2500 - val_accuracy: 0.4933\n",
      "Epoch 54/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.2500 - accuracy: 0.4926 - val_loss: 0.2500 - val_accuracy: 0.4922\n",
      "Epoch 55/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.4981 - val_loss: 0.2500 - val_accuracy: 0.4958\n",
      "Epoch 56/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.4980 - val_loss: 0.2500 - val_accuracy: 0.5019\n",
      "Epoch 57/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.4944 - val_loss: 0.2500 - val_accuracy: 0.5099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.4907 - val_loss: 0.2500 - val_accuracy: 0.5022\n",
      "Epoch 59/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.4931 - val_loss: 0.2500 - val_accuracy: 0.4942\n",
      "Epoch 60/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.4941 - val_loss: 0.2500 - val_accuracy: 0.5062\n",
      "Epoch 61/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.5021 - val_loss: 0.2500 - val_accuracy: 0.4933\n",
      "Epoch 62/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.4883 - val_loss: 0.2500 - val_accuracy: 0.4822\n",
      "Epoch 63/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.4962 - val_loss: 0.2500 - val_accuracy: 0.4950\n",
      "634/634 [==============================] - 1s 2ms/step - loss: 0.2500 - accuracy: 0.4955\n",
      "159/159 [==============================] - 0s 1ms/step\n",
      "634/634 [==============================] - 1s 1ms/step\n",
      "159/159 [==============================] - 0s 927us/step\n",
      "159/159 [==============================] - 0s 1ms/step\n",
      "MLP train Brier score: 0.24999998861161443 \n",
      " test Brier score: 0.24999997526317796 \n",
      " AUROC: 0.5320192548992768\n"
     ]
    }
   ],
   "source": [
    "#(Baseline) Multi Layer Perceptron with Mean Squared Error\n",
    "\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "\n",
    "\n",
    "X_train1,X_test1,y_train1,y_test1=train_test_split(recidivism.drop(['ID','Recidivism_Within_3years','Recidivism_Arrest_Year1','Recidivism_Arrest_Year2','Recidivism_Arrest_Year3'],axis=1),recidivism['Recidivism_Arrest_Year1'],test_size=0.2)\n",
    "fpr_list,tpr_list,auc_list=dict(),dict(),dict()\n",
    "\n",
    "    \n",
    "mse_perceptron = Sequential()\n",
    "n_cols = X_train1.shape[1]\n",
    "mse_perceptron.add(BatchNormalization(input_shape=(n_cols,)))\n",
    "mse_perceptron.add(Dense(70, activation= 'linear'))\n",
    "mse_perceptron.add(Dropout(0.3))\n",
    "mse_perceptron.add(Dense(50, activation= 'relu'))\n",
    "mse_perceptron.add(Dropout(0.3))\n",
    "mse_perceptron.add(Dense(50, activation= 'relu'))\n",
    "mse_perceptron.add(Dropout(0.3))\n",
    "\n",
    "#model.add(Dense(100, activation='relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "#model.add(Dense(100, activation='linear'))\n",
    "mse_perceptron.add(BatchNormalization())\n",
    "mse_perceptron.add(Dense(2, activation='softmax'))\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=50)\n",
    "sgd = keras.optimizers.SGD(lr=.001, decay=2e-4, momentum=0.9, nesterov=True)\n",
    "mse_perceptron.compile(loss = 'mean_squared_error',optimizer='sgd', metrics=['accuracy'])\n",
    "history=mse_perceptron.fit(X_train1.fillna(0).astype('float32'), y_train1,  #normalize data\n",
    "                 validation_split=0.3, epochs=200, callbacks=[early_stopping_monitor])\n",
    "\n",
    "\n",
    "\n",
    "#history=model.fit(X_train, y_train, validation_split=0.2, epochs=25)\n",
    "score = mse_perceptron.evaluate(X_test1.fillna(0).astype('float32'), y_test1, verbose=0)\n",
    "mse_perceptron.fit(X_train1.fillna(0).astype('float32'),y_train1.fillna(0))\n",
    "fpr_list[3], tpr_list[3], _ = roc_curve(y_test1, y_roc(mse_perceptron,X_test1.fillna(0).astype('float32')))\n",
    "print('MLP train Brier score:',\n",
    "      brier_score(mse_perceptron.predict(X_train1.fillna(0).astype('float32')),y_train1),'\\n test Brier score:',brier_score(mse_perceptron.predict(X_test1.fillna(0).astype('float32')),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(mse_perceptron,X_test1.fillna(0).astype('float32'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b067491d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/puddinpop/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444/444 [==============================] - 2s 3ms/step - loss: 0.7092 - accuracy: 0.6377 - val_loss: 0.5643 - val_accuracy: 0.7135\n",
      "Epoch 2/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5949 - accuracy: 0.6950 - val_loss: 0.5589 - val_accuracy: 0.7133\n",
      "Epoch 3/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5796 - accuracy: 0.7002 - val_loss: 0.5554 - val_accuracy: 0.7169\n",
      "Epoch 4/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5714 - accuracy: 0.7060 - val_loss: 0.5520 - val_accuracy: 0.7186\n",
      "Epoch 5/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5694 - accuracy: 0.7070 - val_loss: 0.5505 - val_accuracy: 0.7171\n",
      "Epoch 6/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5695 - accuracy: 0.7082 - val_loss: 0.5484 - val_accuracy: 0.7210\n",
      "Epoch 7/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5629 - accuracy: 0.7099 - val_loss: 0.5454 - val_accuracy: 0.7197\n",
      "Epoch 8/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5624 - accuracy: 0.7133 - val_loss: 0.5415 - val_accuracy: 0.7268\n",
      "Epoch 9/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5600 - accuracy: 0.7126 - val_loss: 0.5409 - val_accuracy: 0.7255\n",
      "Epoch 10/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5583 - accuracy: 0.7176 - val_loss: 0.5396 - val_accuracy: 0.7275\n",
      "Epoch 11/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5579 - accuracy: 0.7164 - val_loss: 0.5405 - val_accuracy: 0.7240\n",
      "Epoch 12/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5547 - accuracy: 0.7157 - val_loss: 0.5388 - val_accuracy: 0.7273\n",
      "Epoch 13/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5554 - accuracy: 0.7135 - val_loss: 0.5385 - val_accuracy: 0.7271\n",
      "Epoch 14/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5561 - accuracy: 0.7180 - val_loss: 0.5379 - val_accuracy: 0.7278\n",
      "Epoch 15/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5522 - accuracy: 0.7182 - val_loss: 0.5373 - val_accuracy: 0.7307\n",
      "Epoch 16/200\n",
      "444/444 [==============================] - 2s 5ms/step - loss: 0.5515 - accuracy: 0.7159 - val_loss: 0.5363 - val_accuracy: 0.7301\n",
      "Epoch 17/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5522 - accuracy: 0.7167 - val_loss: 0.5371 - val_accuracy: 0.7271\n",
      "Epoch 18/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5500 - accuracy: 0.7202 - val_loss: 0.5362 - val_accuracy: 0.7306\n",
      "Epoch 19/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5485 - accuracy: 0.7197 - val_loss: 0.5363 - val_accuracy: 0.7299\n",
      "Epoch 20/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5503 - accuracy: 0.7157 - val_loss: 0.5348 - val_accuracy: 0.7344\n",
      "Epoch 21/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5506 - accuracy: 0.7158 - val_loss: 0.5350 - val_accuracy: 0.7324\n",
      "Epoch 22/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5501 - accuracy: 0.7205 - val_loss: 0.5349 - val_accuracy: 0.7352\n",
      "Epoch 23/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5490 - accuracy: 0.7220 - val_loss: 0.5356 - val_accuracy: 0.7317\n",
      "Epoch 24/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5507 - accuracy: 0.7180 - val_loss: 0.5357 - val_accuracy: 0.7307\n",
      "Epoch 25/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5493 - accuracy: 0.7185 - val_loss: 0.5363 - val_accuracy: 0.7317\n",
      "Epoch 26/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5494 - accuracy: 0.7190 - val_loss: 0.5355 - val_accuracy: 0.7339\n",
      "Epoch 27/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5476 - accuracy: 0.7205 - val_loss: 0.5339 - val_accuracy: 0.7368\n",
      "Epoch 28/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5497 - accuracy: 0.7177 - val_loss: 0.5348 - val_accuracy: 0.7339\n",
      "Epoch 29/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5490 - accuracy: 0.7208 - val_loss: 0.5341 - val_accuracy: 0.7367\n",
      "Epoch 30/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5486 - accuracy: 0.7226 - val_loss: 0.5341 - val_accuracy: 0.7357\n",
      "Epoch 31/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5455 - accuracy: 0.7233 - val_loss: 0.5341 - val_accuracy: 0.7368\n",
      "Epoch 32/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5485 - accuracy: 0.7205 - val_loss: 0.5339 - val_accuracy: 0.7353\n",
      "Epoch 33/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5472 - accuracy: 0.7220 - val_loss: 0.5340 - val_accuracy: 0.7330\n",
      "Epoch 34/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5469 - accuracy: 0.7209 - val_loss: 0.5336 - val_accuracy: 0.7358\n",
      "Epoch 35/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.5473 - accuracy: 0.7204 - val_loss: 0.5333 - val_accuracy: 0.7376\n",
      "Epoch 36/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5451 - accuracy: 0.7234 - val_loss: 0.5333 - val_accuracy: 0.7352\n",
      "Epoch 37/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5451 - accuracy: 0.7220 - val_loss: 0.5330 - val_accuracy: 0.7383\n",
      "Epoch 38/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5468 - accuracy: 0.7202 - val_loss: 0.5346 - val_accuracy: 0.7314\n",
      "Epoch 39/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5439 - accuracy: 0.7229 - val_loss: 0.5334 - val_accuracy: 0.7373\n",
      "Epoch 40/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5455 - accuracy: 0.7233 - val_loss: 0.5335 - val_accuracy: 0.7353\n",
      "Epoch 41/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5470 - accuracy: 0.7176 - val_loss: 0.5344 - val_accuracy: 0.7316\n",
      "Epoch 42/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5441 - accuracy: 0.7242 - val_loss: 0.5351 - val_accuracy: 0.7307\n",
      "Epoch 43/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5461 - accuracy: 0.7231 - val_loss: 0.5330 - val_accuracy: 0.7357\n",
      "Epoch 44/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5449 - accuracy: 0.7228 - val_loss: 0.5338 - val_accuracy: 0.7358\n",
      "Epoch 45/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5462 - accuracy: 0.7216 - val_loss: 0.5333 - val_accuracy: 0.7360\n",
      "Epoch 46/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5444 - accuracy: 0.7224 - val_loss: 0.5338 - val_accuracy: 0.7350\n",
      "Epoch 47/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5449 - accuracy: 0.7220 - val_loss: 0.5331 - val_accuracy: 0.7360\n",
      "Epoch 48/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5451 - accuracy: 0.7247 - val_loss: 0.5350 - val_accuracy: 0.7306\n",
      "Epoch 49/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5447 - accuracy: 0.7216 - val_loss: 0.5335 - val_accuracy: 0.7363\n",
      "Epoch 50/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5446 - accuracy: 0.7225 - val_loss: 0.5332 - val_accuracy: 0.7348\n",
      "Epoch 51/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5429 - accuracy: 0.7250 - val_loss: 0.5332 - val_accuracy: 0.7355\n",
      "Epoch 52/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5431 - accuracy: 0.7219 - val_loss: 0.5327 - val_accuracy: 0.7352\n",
      "Epoch 53/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5437 - accuracy: 0.7228 - val_loss: 0.5329 - val_accuracy: 0.7357\n",
      "Epoch 54/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5442 - accuracy: 0.7251 - val_loss: 0.5331 - val_accuracy: 0.7367\n",
      "Epoch 55/200\n",
      "444/444 [==============================] - 2s 3ms/step - loss: 0.5449 - accuracy: 0.7232 - val_loss: 0.5329 - val_accuracy: 0.7368\n",
      "Epoch 56/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5455 - accuracy: 0.7209 - val_loss: 0.5326 - val_accuracy: 0.7368\n",
      "Epoch 57/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5444 - accuracy: 0.7222 - val_loss: 0.5349 - val_accuracy: 0.7302\n",
      "Epoch 58/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5416 - accuracy: 0.7264 - val_loss: 0.5337 - val_accuracy: 0.7324\n",
      "Epoch 59/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.5457 - accuracy: 0.7239 - val_loss: 0.5330 - val_accuracy: 0.7368\n",
      "Epoch 60/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5462 - accuracy: 0.7230 - val_loss: 0.5329 - val_accuracy: 0.7370\n",
      "Epoch 61/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5459 - accuracy: 0.7233 - val_loss: 0.5327 - val_accuracy: 0.7362\n",
      "Epoch 62/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5440 - accuracy: 0.7259 - val_loss: 0.5322 - val_accuracy: 0.7353\n",
      "Epoch 63/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5447 - accuracy: 0.7226 - val_loss: 0.5328 - val_accuracy: 0.7368\n",
      "Epoch 64/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5463 - accuracy: 0.7223 - val_loss: 0.5324 - val_accuracy: 0.7381\n",
      "Epoch 65/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5423 - accuracy: 0.7245 - val_loss: 0.5329 - val_accuracy: 0.7352\n",
      "Epoch 66/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5425 - accuracy: 0.7214 - val_loss: 0.5325 - val_accuracy: 0.7350\n",
      "Epoch 67/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5433 - accuracy: 0.7209 - val_loss: 0.5322 - val_accuracy: 0.7363\n",
      "Epoch 68/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5432 - accuracy: 0.7228 - val_loss: 0.5320 - val_accuracy: 0.7357\n",
      "Epoch 69/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5411 - accuracy: 0.7256 - val_loss: 0.5324 - val_accuracy: 0.7362\n",
      "Epoch 70/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5425 - accuracy: 0.7254 - val_loss: 0.5333 - val_accuracy: 0.7321\n",
      "Epoch 71/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5434 - accuracy: 0.7203 - val_loss: 0.5325 - val_accuracy: 0.7363\n",
      "Epoch 72/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5442 - accuracy: 0.7238 - val_loss: 0.5325 - val_accuracy: 0.7370\n",
      "Epoch 73/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5444 - accuracy: 0.7235 - val_loss: 0.5323 - val_accuracy: 0.7370\n",
      "Epoch 74/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5428 - accuracy: 0.7244 - val_loss: 0.5319 - val_accuracy: 0.7353\n",
      "Epoch 75/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5420 - accuracy: 0.7236 - val_loss: 0.5318 - val_accuracy: 0.7365\n",
      "Epoch 76/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5399 - accuracy: 0.7295 - val_loss: 0.5317 - val_accuracy: 0.7373\n",
      "Epoch 77/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5438 - accuracy: 0.7239 - val_loss: 0.5315 - val_accuracy: 0.7403\n",
      "Epoch 78/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5414 - accuracy: 0.7221 - val_loss: 0.5319 - val_accuracy: 0.7370\n",
      "Epoch 79/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5442 - accuracy: 0.7232 - val_loss: 0.5321 - val_accuracy: 0.7375\n",
      "Epoch 80/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5421 - accuracy: 0.7272 - val_loss: 0.5323 - val_accuracy: 0.7332\n",
      "Epoch 81/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5413 - accuracy: 0.7240 - val_loss: 0.5316 - val_accuracy: 0.7373\n",
      "Epoch 82/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5442 - accuracy: 0.7236 - val_loss: 0.5319 - val_accuracy: 0.7371\n",
      "Epoch 83/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5418 - accuracy: 0.7232 - val_loss: 0.5317 - val_accuracy: 0.7406\n",
      "Epoch 84/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5422 - accuracy: 0.7252 - val_loss: 0.5318 - val_accuracy: 0.7357\n",
      "Epoch 85/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5413 - accuracy: 0.7259 - val_loss: 0.5318 - val_accuracy: 0.7342\n",
      "Epoch 86/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5413 - accuracy: 0.7255 - val_loss: 0.5315 - val_accuracy: 0.7381\n",
      "Epoch 87/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5410 - accuracy: 0.7257 - val_loss: 0.5315 - val_accuracy: 0.7413\n",
      "Epoch 88/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5450 - accuracy: 0.7214 - val_loss: 0.5318 - val_accuracy: 0.7376\n",
      "Epoch 89/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5417 - accuracy: 0.7236 - val_loss: 0.5318 - val_accuracy: 0.7380\n",
      "Epoch 90/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5412 - accuracy: 0.7283 - val_loss: 0.5322 - val_accuracy: 0.7339\n",
      "Epoch 91/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5436 - accuracy: 0.7251 - val_loss: 0.5314 - val_accuracy: 0.7378\n",
      "Epoch 92/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5427 - accuracy: 0.7259 - val_loss: 0.5318 - val_accuracy: 0.7391\n",
      "Epoch 93/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5430 - accuracy: 0.7258 - val_loss: 0.5321 - val_accuracy: 0.7347\n",
      "Epoch 94/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5424 - accuracy: 0.7238 - val_loss: 0.5318 - val_accuracy: 0.7355\n",
      "Epoch 95/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5449 - accuracy: 0.7215 - val_loss: 0.5329 - val_accuracy: 0.7329\n",
      "Epoch 96/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5415 - accuracy: 0.7226 - val_loss: 0.5315 - val_accuracy: 0.7365\n",
      "Epoch 97/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5435 - accuracy: 0.7268 - val_loss: 0.5316 - val_accuracy: 0.7362\n",
      "Epoch 98/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5399 - accuracy: 0.7251 - val_loss: 0.5320 - val_accuracy: 0.7352\n",
      "Epoch 99/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5421 - accuracy: 0.7233 - val_loss: 0.5318 - val_accuracy: 0.7363\n",
      "Epoch 100/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5412 - accuracy: 0.7279 - val_loss: 0.5318 - val_accuracy: 0.7376\n",
      "Epoch 101/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5427 - accuracy: 0.7273 - val_loss: 0.5313 - val_accuracy: 0.7388\n",
      "Epoch 102/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5412 - accuracy: 0.7235 - val_loss: 0.5315 - val_accuracy: 0.7390\n",
      "Epoch 103/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5430 - accuracy: 0.7234 - val_loss: 0.5313 - val_accuracy: 0.7383\n",
      "Epoch 104/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5423 - accuracy: 0.7234 - val_loss: 0.5329 - val_accuracy: 0.7334\n",
      "Epoch 105/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5406 - accuracy: 0.7294 - val_loss: 0.5328 - val_accuracy: 0.7330\n",
      "Epoch 106/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5419 - accuracy: 0.7252 - val_loss: 0.5324 - val_accuracy: 0.7358\n",
      "Epoch 107/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5434 - accuracy: 0.7271 - val_loss: 0.5312 - val_accuracy: 0.7373\n",
      "Epoch 108/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5400 - accuracy: 0.7271 - val_loss: 0.5314 - val_accuracy: 0.7399\n",
      "Epoch 109/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5400 - accuracy: 0.7254 - val_loss: 0.5314 - val_accuracy: 0.7376\n",
      "Epoch 110/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5426 - accuracy: 0.7240 - val_loss: 0.5316 - val_accuracy: 0.7391\n",
      "Epoch 111/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5405 - accuracy: 0.7249 - val_loss: 0.5313 - val_accuracy: 0.7390\n",
      "Epoch 112/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5423 - accuracy: 0.7272 - val_loss: 0.5316 - val_accuracy: 0.7383\n",
      "Epoch 113/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5397 - accuracy: 0.7302 - val_loss: 0.5312 - val_accuracy: 0.7411\n",
      "Epoch 114/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5393 - accuracy: 0.7266 - val_loss: 0.5313 - val_accuracy: 0.7413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5424 - accuracy: 0.7250 - val_loss: 0.5314 - val_accuracy: 0.7362\n",
      "Epoch 116/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5406 - accuracy: 0.7240 - val_loss: 0.5310 - val_accuracy: 0.7360\n",
      "Epoch 117/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5398 - accuracy: 0.7248 - val_loss: 0.5317 - val_accuracy: 0.7371\n",
      "Epoch 118/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5433 - accuracy: 0.7212 - val_loss: 0.5316 - val_accuracy: 0.7401\n",
      "Epoch 119/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5419 - accuracy: 0.7233 - val_loss: 0.5312 - val_accuracy: 0.7375\n",
      "Epoch 120/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5385 - accuracy: 0.7288 - val_loss: 0.5322 - val_accuracy: 0.7350\n",
      "Epoch 121/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5404 - accuracy: 0.7254 - val_loss: 0.5320 - val_accuracy: 0.7360\n",
      "Epoch 122/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5398 - accuracy: 0.7242 - val_loss: 0.5315 - val_accuracy: 0.7383\n",
      "Epoch 123/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5430 - accuracy: 0.7252 - val_loss: 0.5357 - val_accuracy: 0.7265\n",
      "Epoch 124/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5421 - accuracy: 0.7275 - val_loss: 0.5316 - val_accuracy: 0.7390\n",
      "Epoch 125/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5415 - accuracy: 0.7250 - val_loss: 0.5311 - val_accuracy: 0.7383\n",
      "Epoch 126/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5416 - accuracy: 0.7242 - val_loss: 0.5329 - val_accuracy: 0.7342\n",
      "Epoch 127/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5421 - accuracy: 0.7252 - val_loss: 0.5318 - val_accuracy: 0.7353\n",
      "Epoch 128/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5411 - accuracy: 0.7279 - val_loss: 0.5311 - val_accuracy: 0.7391\n",
      "Epoch 129/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5413 - accuracy: 0.7244 - val_loss: 0.5310 - val_accuracy: 0.7399\n",
      "Epoch 130/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5408 - accuracy: 0.7254 - val_loss: 0.5316 - val_accuracy: 0.7376\n",
      "Epoch 131/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5419 - accuracy: 0.7211 - val_loss: 0.5311 - val_accuracy: 0.7393\n",
      "Epoch 132/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5392 - accuracy: 0.7278 - val_loss: 0.5321 - val_accuracy: 0.7344\n",
      "Epoch 133/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5402 - accuracy: 0.7255 - val_loss: 0.5321 - val_accuracy: 0.7371\n",
      "Epoch 134/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5399 - accuracy: 0.7253 - val_loss: 0.5305 - val_accuracy: 0.7403\n",
      "Epoch 135/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5400 - accuracy: 0.7288 - val_loss: 0.5315 - val_accuracy: 0.7375\n",
      "Epoch 136/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5434 - accuracy: 0.7218 - val_loss: 0.5326 - val_accuracy: 0.7350\n",
      "Epoch 137/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5407 - accuracy: 0.7250 - val_loss: 0.5311 - val_accuracy: 0.7368\n",
      "Epoch 138/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5403 - accuracy: 0.7266 - val_loss: 0.5310 - val_accuracy: 0.7383\n",
      "Epoch 139/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5406 - accuracy: 0.7260 - val_loss: 0.5321 - val_accuracy: 0.7337\n",
      "Epoch 140/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5403 - accuracy: 0.7243 - val_loss: 0.5304 - val_accuracy: 0.7398\n",
      "Epoch 141/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5410 - accuracy: 0.7238 - val_loss: 0.5305 - val_accuracy: 0.7391\n",
      "Epoch 142/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5408 - accuracy: 0.7266 - val_loss: 0.5312 - val_accuracy: 0.7386\n",
      "Epoch 143/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5415 - accuracy: 0.7269 - val_loss: 0.5312 - val_accuracy: 0.7411\n",
      "Epoch 144/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5407 - accuracy: 0.7262 - val_loss: 0.5306 - val_accuracy: 0.7413\n",
      "Epoch 145/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5409 - accuracy: 0.7242 - val_loss: 0.5306 - val_accuracy: 0.7409\n",
      "Epoch 146/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5398 - accuracy: 0.7276 - val_loss: 0.5313 - val_accuracy: 0.7360\n",
      "Epoch 147/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5397 - accuracy: 0.7264 - val_loss: 0.5307 - val_accuracy: 0.7416\n",
      "Epoch 148/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5419 - accuracy: 0.7252 - val_loss: 0.5311 - val_accuracy: 0.7373\n",
      "Epoch 149/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5411 - accuracy: 0.7263 - val_loss: 0.5312 - val_accuracy: 0.7358\n",
      "Epoch 150/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5404 - accuracy: 0.7242 - val_loss: 0.5300 - val_accuracy: 0.7413\n",
      "Epoch 151/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5406 - accuracy: 0.7262 - val_loss: 0.5310 - val_accuracy: 0.7388\n",
      "Epoch 152/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5409 - accuracy: 0.7265 - val_loss: 0.5308 - val_accuracy: 0.7401\n",
      "Epoch 153/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5420 - accuracy: 0.7253 - val_loss: 0.5310 - val_accuracy: 0.7353\n",
      "Epoch 154/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5413 - accuracy: 0.7278 - val_loss: 0.5306 - val_accuracy: 0.7378\n",
      "Epoch 155/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5389 - accuracy: 0.7265 - val_loss: 0.5301 - val_accuracy: 0.7398\n",
      "Epoch 156/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5398 - accuracy: 0.7273 - val_loss: 0.5309 - val_accuracy: 0.7353\n",
      "Epoch 157/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5390 - accuracy: 0.7271 - val_loss: 0.5311 - val_accuracy: 0.7355\n",
      "Epoch 158/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5395 - accuracy: 0.7256 - val_loss: 0.5304 - val_accuracy: 0.7396\n",
      "Epoch 159/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5421 - accuracy: 0.7276 - val_loss: 0.5302 - val_accuracy: 0.7413\n",
      "Epoch 160/200\n",
      "444/444 [==============================] - 2s 3ms/step - loss: 0.5386 - accuracy: 0.7271 - val_loss: 0.5309 - val_accuracy: 0.7381\n",
      "Epoch 161/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5371 - accuracy: 0.7293 - val_loss: 0.5303 - val_accuracy: 0.7417\n",
      "Epoch 162/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5392 - accuracy: 0.7268 - val_loss: 0.5329 - val_accuracy: 0.7319\n",
      "Epoch 163/200\n",
      "444/444 [==============================] - 2s 5ms/step - loss: 0.5384 - accuracy: 0.7299 - val_loss: 0.5311 - val_accuracy: 0.7352\n",
      "Epoch 164/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.5392 - accuracy: 0.7268 - val_loss: 0.5316 - val_accuracy: 0.7352\n",
      "Epoch 165/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.5401 - accuracy: 0.7242 - val_loss: 0.5301 - val_accuracy: 0.7385\n",
      "Epoch 166/200\n",
      "444/444 [==============================] - 2s 5ms/step - loss: 0.5398 - accuracy: 0.7271 - val_loss: 0.5322 - val_accuracy: 0.7327\n",
      "Epoch 167/200\n",
      "444/444 [==============================] - 2s 5ms/step - loss: 0.5376 - accuracy: 0.7295 - val_loss: 0.5305 - val_accuracy: 0.7371\n",
      "Epoch 168/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.5393 - accuracy: 0.7287 - val_loss: 0.5322 - val_accuracy: 0.7312\n",
      "Epoch 169/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5373 - accuracy: 0.7304 - val_loss: 0.5311 - val_accuracy: 0.7365\n",
      "Epoch 170/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5377 - accuracy: 0.7276 - val_loss: 0.5304 - val_accuracy: 0.7401\n",
      "Epoch 171/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5388 - accuracy: 0.7273 - val_loss: 0.5300 - val_accuracy: 0.7409\n",
      "Epoch 172/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5394 - accuracy: 0.7244 - val_loss: 0.5326 - val_accuracy: 0.7316\n",
      "Epoch 173/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5409 - accuracy: 0.7254 - val_loss: 0.5307 - val_accuracy: 0.7376\n",
      "Epoch 174/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5374 - accuracy: 0.7263 - val_loss: 0.5302 - val_accuracy: 0.7426\n",
      "Epoch 175/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5382 - accuracy: 0.7270 - val_loss: 0.5305 - val_accuracy: 0.7368\n",
      "Epoch 176/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5397 - accuracy: 0.7254 - val_loss: 0.5304 - val_accuracy: 0.7408\n",
      "Epoch 177/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5407 - accuracy: 0.7267 - val_loss: 0.5311 - val_accuracy: 0.7352\n",
      "Epoch 178/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5385 - accuracy: 0.7281 - val_loss: 0.5299 - val_accuracy: 0.7417\n",
      "Epoch 179/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5381 - accuracy: 0.7290 - val_loss: 0.5299 - val_accuracy: 0.7391\n",
      "Epoch 180/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5403 - accuracy: 0.7285 - val_loss: 0.5301 - val_accuracy: 0.7413\n",
      "Epoch 181/200\n",
      "444/444 [==============================] - 2s 3ms/step - loss: 0.5414 - accuracy: 0.7283 - val_loss: 0.5303 - val_accuracy: 0.7416\n",
      "Epoch 182/200\n",
      "444/444 [==============================] - 2s 4ms/step - loss: 0.5392 - accuracy: 0.7276 - val_loss: 0.5303 - val_accuracy: 0.7396\n",
      "Epoch 183/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5392 - accuracy: 0.7272 - val_loss: 0.5309 - val_accuracy: 0.7380\n",
      "Epoch 184/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5384 - accuracy: 0.7282 - val_loss: 0.5302 - val_accuracy: 0.7416\n",
      "Epoch 185/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5385 - accuracy: 0.7266 - val_loss: 0.5307 - val_accuracy: 0.7373\n",
      "Epoch 186/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5376 - accuracy: 0.7266 - val_loss: 0.5301 - val_accuracy: 0.7394\n",
      "Epoch 187/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5381 - accuracy: 0.7278 - val_loss: 0.5305 - val_accuracy: 0.7370\n",
      "Epoch 188/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5401 - accuracy: 0.7273 - val_loss: 0.5302 - val_accuracy: 0.7399\n",
      "Epoch 189/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5389 - accuracy: 0.7280 - val_loss: 0.5300 - val_accuracy: 0.7409\n",
      "Epoch 190/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5404 - accuracy: 0.7267 - val_loss: 0.5300 - val_accuracy: 0.7416\n",
      "Epoch 191/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5379 - accuracy: 0.7287 - val_loss: 0.5305 - val_accuracy: 0.7385\n",
      "Epoch 192/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5396 - accuracy: 0.7273 - val_loss: 0.5298 - val_accuracy: 0.7393\n",
      "Epoch 193/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5375 - accuracy: 0.7307 - val_loss: 0.5300 - val_accuracy: 0.7408\n",
      "Epoch 194/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5375 - accuracy: 0.7273 - val_loss: 0.5302 - val_accuracy: 0.7386\n",
      "Epoch 195/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5372 - accuracy: 0.7264 - val_loss: 0.5307 - val_accuracy: 0.7373\n",
      "Epoch 196/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5387 - accuracy: 0.7234 - val_loss: 0.5308 - val_accuracy: 0.7388\n",
      "Epoch 197/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5376 - accuracy: 0.7243 - val_loss: 0.5320 - val_accuracy: 0.7317\n",
      "Epoch 198/200\n",
      "444/444 [==============================] - 1s 2ms/step - loss: 0.5386 - accuracy: 0.7268 - val_loss: 0.5301 - val_accuracy: 0.7375\n",
      "Epoch 199/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5379 - accuracy: 0.7294 - val_loss: 0.5310 - val_accuracy: 0.7347\n",
      "Epoch 200/200\n",
      "444/444 [==============================] - 1s 3ms/step - loss: 0.5382 - accuracy: 0.7286 - val_loss: 0.5299 - val_accuracy: 0.7383\n",
      "634/634 [==============================] - 1s 2ms/step - loss: 0.5402 - accuracy: 0.7261\n",
      "159/159 [==============================] - 0s 954us/step\n",
      "634/634 [==============================] - 1s 1ms/step\n",
      "159/159 [==============================] - 0s 1ms/step\n",
      "159/159 [==============================] - 0s 1ms/step\n",
      "MLP train Brier score: 0.1767800450590889 \n",
      " test Brier score: 0.18114597255560275 \n",
      " AUROC: 0.7378915429014669\n"
     ]
    }
   ],
   "source": [
    "#Multi Layer Perceptron from Categorical Cross Entropy\n",
    "    \n",
    "cce_perceptron = Sequential()\n",
    "n_cols = X_train1.shape[1]\n",
    "cce_perceptron.add(BatchNormalization(input_shape=(n_cols,)))\n",
    "cce_perceptron.add(Dense(70, activation= 'linear'))\n",
    "cce_perceptron.add(Dropout(0.3))\n",
    "cce_perceptron.add(Dense(50, activation= 'relu'))\n",
    "cce_perceptron.add(Dropout(0.3))\n",
    "cce_perceptron.add(Dense(50, activation= 'relu'))\n",
    "cce_perceptron.add(Dropout(0.3))\n",
    "\n",
    "#model.add(Dense(100, activation='relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "#model.add(Dense(100, activation='linear'))\n",
    "cce_perceptron.add(BatchNormalization())\n",
    "cce_perceptron.add(Dense(2, activation='softmax'))\n",
    "    #model.compile(\n",
    "        #optimizer='Adam',\n",
    "        \n",
    "   \n",
    "early_stopping_monitor = EarlyStopping(patience=50)\n",
    "sgd = keras.optimizers.SGD(lr=.001, decay=2e-4, momentum=0.9, nesterov=True)\n",
    "cce_perceptron.compile(loss = 'sparse_categorical_crossentropy',optimizer = 'sgd', metrics=['accuracy'])\n",
    "history=cce_perceptron.fit(X_train1.fillna(0).astype('float32'), y_train1,  #normalize data\n",
    "                 validation_split=0.3, epochs=200, callbacks=[early_stopping_monitor])\n",
    "\n",
    "\n",
    "\n",
    "#history=model.fit(X_train, y_train, validation_split=0.2, epochs=25)\n",
    "score = cce_perceptron.evaluate(X_test1.fillna(0).astype('float32'), y_test1, verbose=0)\n",
    "cce_perceptron.fit(X_train1.fillna(0).astype('float32'),y_train1.fillna(0))\n",
    "fpr_list[3], tpr_list[3], _ = roc_curve(y_test1, y_roc(cce_perceptron,X_test1.fillna(0).astype('float32')))\n",
    "print('MLP train Brier score:',\n",
    "      brier_score(cce_perceptron.predict(X_train1.fillna(0).astype('float32')),y_train1),'\\n test Brier score:',brier_score(cce_perceptron.predict(X_test1.fillna(0).astype('float32')),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(cce_perceptron,X_test1.fillna(0).astype('float32'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "da7f6227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/puddinpop/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444/444 [==============================] - 4s 7ms/step - loss: 0.6938 - accuracy: 0.6364 - val_loss: 0.5515 - val_accuracy: 0.7276\n",
      "Epoch 2/50\n",
      "444/444 [==============================] - 2s 6ms/step - loss: 0.5920 - accuracy: 0.6881 - val_loss: 0.5584 - val_accuracy: 0.7110\n",
      "Epoch 3/50\n",
      "444/444 [==============================] - 3s 6ms/step - loss: 0.5714 - accuracy: 0.7040 - val_loss: 0.5354 - val_accuracy: 0.7373\n",
      "Epoch 4/50\n",
      "444/444 [==============================] - 3s 6ms/step - loss: 0.5640 - accuracy: 0.7145 - val_loss: 0.5361 - val_accuracy: 0.7340\n",
      "Epoch 5/50\n",
      "444/444 [==============================] - 3s 6ms/step - loss: 0.5563 - accuracy: 0.7171 - val_loss: 0.5338 - val_accuracy: 0.7367\n",
      "Epoch 6/50\n",
      "444/444 [==============================] - 3s 8ms/step - loss: 0.5573 - accuracy: 0.7128 - val_loss: 0.5336 - val_accuracy: 0.7376\n",
      "Epoch 7/50\n",
      "444/444 [==============================] - 3s 6ms/step - loss: 0.5562 - accuracy: 0.7161 - val_loss: 0.5345 - val_accuracy: 0.7409\n",
      "Epoch 8/50\n",
      "444/444 [==============================] - 3s 6ms/step - loss: 0.5559 - accuracy: 0.7175 - val_loss: 0.5337 - val_accuracy: 0.7340\n",
      "Epoch 9/50\n",
      "444/444 [==============================] - 3s 6ms/step - loss: 0.5503 - accuracy: 0.7203 - val_loss: 0.5324 - val_accuracy: 0.7399\n",
      "Epoch 10/50\n",
      "444/444 [==============================] - 3s 6ms/step - loss: 0.5477 - accuracy: 0.7219 - val_loss: 0.5315 - val_accuracy: 0.7353\n",
      "Epoch 11/50\n",
      "444/444 [==============================] - 3s 6ms/step - loss: 0.5526 - accuracy: 0.7156 - val_loss: 0.5312 - val_accuracy: 0.7386\n",
      "Epoch 12/50\n",
      "444/444 [==============================] - 2s 6ms/step - loss: 0.5473 - accuracy: 0.7227 - val_loss: 0.5319 - val_accuracy: 0.7394\n",
      "Epoch 13/50\n",
      "444/444 [==============================] - 3s 6ms/step - loss: 0.5487 - accuracy: 0.7199 - val_loss: 0.5310 - val_accuracy: 0.7409\n",
      "Epoch 14/50\n",
      "444/444 [==============================] - 3s 6ms/step - loss: 0.5445 - accuracy: 0.7235 - val_loss: 0.5325 - val_accuracy: 0.7432\n",
      "Epoch 15/50\n",
      "444/444 [==============================] - 3s 6ms/step - loss: 0.5449 - accuracy: 0.7261 - val_loss: 0.5341 - val_accuracy: 0.7347\n",
      "Epoch 16/50\n",
      "444/444 [==============================] - 3s 8ms/step - loss: 0.5486 - accuracy: 0.7211 - val_loss: 0.5309 - val_accuracy: 0.7363\n",
      "Epoch 17/50\n",
      "444/444 [==============================] - 3s 8ms/step - loss: 0.5444 - accuracy: 0.7231 - val_loss: 0.5312 - val_accuracy: 0.7344\n",
      "Epoch 18/50\n",
      "444/444 [==============================] - 3s 6ms/step - loss: 0.5436 - accuracy: 0.7245 - val_loss: 0.5299 - val_accuracy: 0.7394\n",
      "Epoch 19/50\n",
      "444/444 [==============================] - 3s 6ms/step - loss: 0.5442 - accuracy: 0.7238 - val_loss: 0.5308 - val_accuracy: 0.7342\n",
      "Epoch 20/50\n",
      "444/444 [==============================] - 3s 8ms/step - loss: 0.5451 - accuracy: 0.7239 - val_loss: 0.5343 - val_accuracy: 0.7342\n",
      "Epoch 21/50\n",
      "444/444 [==============================] - 4s 9ms/step - loss: 0.5428 - accuracy: 0.7224 - val_loss: 0.5307 - val_accuracy: 0.7388\n",
      "Epoch 22/50\n",
      "444/444 [==============================] - 3s 7ms/step - loss: 0.5404 - accuracy: 0.7291 - val_loss: 0.5332 - val_accuracy: 0.7370\n",
      "Epoch 23/50\n",
      "444/444 [==============================] - 3s 7ms/step - loss: 0.5435 - accuracy: 0.7281 - val_loss: 0.5292 - val_accuracy: 0.7406\n",
      "Epoch 24/50\n",
      "444/444 [==============================] - 3s 7ms/step - loss: 0.5423 - accuracy: 0.7260 - val_loss: 0.5322 - val_accuracy: 0.7408\n",
      "Epoch 25/50\n",
      "444/444 [==============================] - 4s 10ms/step - loss: 0.5385 - accuracy: 0.7305 - val_loss: 0.5344 - val_accuracy: 0.7324\n",
      "Epoch 26/50\n",
      "444/444 [==============================] - 4s 8ms/step - loss: 0.5402 - accuracy: 0.7283 - val_loss: 0.5282 - val_accuracy: 0.7390\n",
      "Epoch 27/50\n",
      "444/444 [==============================] - 3s 8ms/step - loss: 0.5387 - accuracy: 0.7302 - val_loss: 0.5304 - val_accuracy: 0.7391\n",
      "Epoch 28/50\n",
      "444/444 [==============================] - 3s 7ms/step - loss: 0.5400 - accuracy: 0.7294 - val_loss: 0.5292 - val_accuracy: 0.7417\n",
      "Epoch 29/50\n",
      "444/444 [==============================] - 3s 7ms/step - loss: 0.5361 - accuracy: 0.7350 - val_loss: 0.5288 - val_accuracy: 0.7421\n",
      "Epoch 30/50\n",
      "444/444 [==============================] - 3s 6ms/step - loss: 0.5363 - accuracy: 0.7306 - val_loss: 0.5301 - val_accuracy: 0.7376\n",
      "Epoch 31/50\n",
      "444/444 [==============================] - 3s 6ms/step - loss: 0.5378 - accuracy: 0.7304 - val_loss: 0.5336 - val_accuracy: 0.7424\n",
      "Epoch 32/50\n",
      "444/444 [==============================] - 3s 6ms/step - loss: 0.5371 - accuracy: 0.7302 - val_loss: 0.5293 - val_accuracy: 0.7406\n",
      "Epoch 33/50\n",
      "444/444 [==============================] - 3s 7ms/step - loss: 0.5357 - accuracy: 0.7304 - val_loss: 0.5351 - val_accuracy: 0.7348\n",
      "Epoch 34/50\n",
      "444/444 [==============================] - 2s 6ms/step - loss: 0.5346 - accuracy: 0.7316 - val_loss: 0.5295 - val_accuracy: 0.7381\n",
      "Epoch 35/50\n",
      "444/444 [==============================] - 3s 7ms/step - loss: 0.5338 - accuracy: 0.7357 - val_loss: 0.5298 - val_accuracy: 0.7416\n",
      "Epoch 36/50\n",
      "444/444 [==============================] - 4s 8ms/step - loss: 0.5342 - accuracy: 0.7326 - val_loss: 0.5326 - val_accuracy: 0.7409\n",
      "Epoch 37/50\n",
      "444/444 [==============================] - 3s 6ms/step - loss: 0.5315 - accuracy: 0.7340 - val_loss: 0.5352 - val_accuracy: 0.7358\n",
      "Epoch 38/50\n",
      "444/444 [==============================] - 3s 8ms/step - loss: 0.5344 - accuracy: 0.7320 - val_loss: 0.5307 - val_accuracy: 0.7417\n",
      "Epoch 39/50\n",
      "444/444 [==============================] - 3s 8ms/step - loss: 0.5328 - accuracy: 0.7357 - val_loss: 0.5312 - val_accuracy: 0.7393\n",
      "Epoch 40/50\n",
      "444/444 [==============================] - 4s 8ms/step - loss: 0.5311 - accuracy: 0.7364 - val_loss: 0.5330 - val_accuracy: 0.7426\n",
      "Epoch 41/50\n",
      "444/444 [==============================] - 3s 8ms/step - loss: 0.5330 - accuracy: 0.7355 - val_loss: 0.5339 - val_accuracy: 0.7394\n",
      "Epoch 42/50\n",
      "444/444 [==============================] - 4s 10ms/step - loss: 0.5298 - accuracy: 0.7380 - val_loss: 0.5297 - val_accuracy: 0.7413\n",
      "Epoch 43/50\n",
      "444/444 [==============================] - 5s 12ms/step - loss: 0.5328 - accuracy: 0.7381 - val_loss: 0.5316 - val_accuracy: 0.7401\n",
      "Epoch 44/50\n",
      "444/444 [==============================] - 4s 8ms/step - loss: 0.5314 - accuracy: 0.7353 - val_loss: 0.5293 - val_accuracy: 0.7391\n",
      "Epoch 45/50\n",
      "444/444 [==============================] - 3s 6ms/step - loss: 0.5299 - accuracy: 0.7402 - val_loss: 0.5288 - val_accuracy: 0.7380\n",
      "Epoch 46/50\n",
      "444/444 [==============================] - 3s 7ms/step - loss: 0.5286 - accuracy: 0.7354 - val_loss: 0.5318 - val_accuracy: 0.7409\n",
      "Epoch 47/50\n",
      "444/444 [==============================] - 3s 6ms/step - loss: 0.5305 - accuracy: 0.7368 - val_loss: 0.5355 - val_accuracy: 0.7352\n",
      "Epoch 48/50\n",
      "444/444 [==============================] - 3s 6ms/step - loss: 0.5320 - accuracy: 0.7388 - val_loss: 0.5317 - val_accuracy: 0.7403\n",
      "Epoch 49/50\n",
      "444/444 [==============================] - 3s 7ms/step - loss: 0.5273 - accuracy: 0.7399 - val_loss: 0.5372 - val_accuracy: 0.7393\n",
      "Epoch 50/50\n",
      "444/444 [==============================] - 3s 6ms/step - loss: 0.5304 - accuracy: 0.7390 - val_loss: 0.5333 - val_accuracy: 0.7404\n",
      "634/634 [==============================] - 4s 6ms/step - loss: 0.5316 - accuracy: 0.7347\n",
      "159/159 [==============================] - 0s 2ms/step\n",
      "634/634 [==============================] - 2s 3ms/step\n",
      "159/159 [==============================] - 1s 3ms/step\n",
      "159/159 [==============================] - 1s 4ms/step\n",
      "MLP train Brier score: 0.17074229545307656 \n",
      " test Brier score: 0.18099677535531233 \n",
      " AUROC: 0.7377625709897462\n"
     ]
    }
   ],
   "source": [
    "#Multi Layer Perceptron from Categorical Cross Entropy\n",
    "    \n",
    "cce_perceptron = Sequential()\n",
    "n_cols = X_train1.shape[1]\n",
    "cce_perceptron.add(BatchNormalization(input_shape=(n_cols,)))\n",
    "cce_perceptron.add(Dense(1024, activation= 'relu'))\n",
    "cce_perceptron.add(Dropout(0.5))\n",
    "cce_perceptron.add(Dense(256, activation= 'relu'))\n",
    "cce_perceptron.add(Dense(256, activation= 'relu'))\n",
    "cce_perceptron.add(Dropout(0.5))\n",
    "cce_perceptron.add(Dense(500, activation= 'relu'))\n",
    "cce_perceptron.add(Dropout(0.3))\n",
    "\n",
    "#model.add(Dense(100, activation='relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "#model.add(Dense(100, activation='linear'))\n",
    "cce_perceptron.add(BatchNormalization())\n",
    "cce_perceptron.add(Dense(2, activation='softmax'))\n",
    "    #model.compile(\n",
    "        #optimizer='Adam',\n",
    "        \n",
    "   \n",
    "early_stopping_monitor = EarlyStopping(patience=50)\n",
    "sgd = keras.optimizers.SGD(lr=.001, decay=2e-4, momentum=0.9, nesterov=True)\n",
    "cce_perceptron.compile(loss = 'sparse_categorical_crossentropy',optimizer = 'sgd', metrics=['accuracy'])\n",
    "history=cce_perceptron.fit(X_train1.fillna(0).astype('float32'), y_train1,  #normalize data\n",
    "                 validation_split=0.3, epochs=50, callbacks=[early_stopping_monitor])\n",
    "\n",
    "\n",
    "\n",
    "#history=model.fit(X_train, y_train, validation_split=0.2, epochs=25)\n",
    "score = cce_perceptron.evaluate(X_test1.fillna(0).astype('float32'), y_test1, verbose=0)\n",
    "cce_perceptron.fit(X_train1.fillna(0).astype('float32'),y_train1.fillna(0))\n",
    "fpr_list[3], tpr_list[3], _ = roc_curve(y_test1, y_roc(cce_perceptron,X_test1.fillna(0).astype('float32')))\n",
    "print('MLP train Brier score:',\n",
    "      brier_score(cce_perceptron.predict(X_train1.fillna(0).astype('float32')),y_train1),'\\n test Brier score:',brier_score(cce_perceptron.predict(X_test1.fillna(0).astype('float32')),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(cce_perceptron,X_test1.fillna(0).astype('float32'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17d30e1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/h_/hcwngb6120b_wyhn1trrf8x40000gn/T/ipykernel_9620/1257447944.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Multi Layer Perceptron from Categorical Cross Entropy and Optimizer Adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcce_adam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mn_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "#Multi Layer Perceptron from Categorical Cross Entropy and Optimizer Adam\n",
    "    \n",
    "cce_adam = Sequential()\n",
    "\n",
    "n_cols = X_train1.shape[1]\n",
    "\n",
    "cce_adam.add(BatchNormalization(input_shape=(n_cols,)))\n",
    "cce_adam.add(Dense(70, activation= 'linear'))\n",
    "cce_adam.add(Dropout(0.3))\n",
    "cce_adam.add(Dense(50, activation= 'relu'))\n",
    "cce_adam.add(Dropout(0.3))\n",
    "cce_adam.add(Dense(50, activation= 'relu'))\n",
    "cce_adam.add(Dropout(0.3))\n",
    "\n",
    "#model.add(Dense(100, activation='relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "#model.add(Dense(100, activation='linear'))\n",
    "cce_adam.add(BatchNormalization())\n",
    "cce_adam.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "        \n",
    "   \n",
    "early_stopping_monitor = EarlyStopping(patience=50)\n",
    "adam = keras.optimizers.Adam(lr=.001, decay=2e-4)\n",
    "cce_adam.compile(loss = 'sparse_categorical_crossentropy',optimizer = 'adam', metrics=['accuracy'])\n",
    "history=cce_adam.fit(X_train1.fillna(0).astype('float32'), y_train1,  #normalize data\n",
    "                 validation_split=0.3, epochs=200, callbacks=[early_stopping_monitor])\n",
    "\n",
    "\n",
    "\n",
    "#history=model.fit(X_train, y_train, validation_split=0.2, epochs=25)\n",
    "score = cce_adam.evaluate(X_test1.fillna(0).astype('float32'), y_test1, verbose=0)\n",
    "cce_adam.fit(X_train1.fillna(0).astype('float32'),y_train1.fillna(0))\n",
    "fpr_list[3], tpr_list[3], _ = roc_curve(y_test1, y_roc(cce_adam,X_test1.fillna(0).astype('float32')))\n",
    "print('MLP train Brier score:',\n",
    "      brier_score(cce_adam.predict(X_train1.fillna(0).astype('float32')),y_train1),'\\n test Brier score:',brier_score(cce_adam.predict(X_test1.fillna(0).astype('float32')),y_test1),\n",
    "     '\\n AUROC:',roc_auc_score(y_test1, y_roc(cce_adam,X_test1.fillna(0).astype('float32'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddda481",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "562dbcf3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'recidivism' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/h_/hcwngb6120b_wyhn1trrf8x40000gn/T/ipykernel_9620/162821830.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtestdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecidivism\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0m__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'recidivism' is not defined"
     ]
    }
   ],
   "source": [
    "testdata = recidivism(X_test_np, y_test_np)\n",
    "outputs = clf(testdata.X)\n",
    "__, predicted = torch.max(outputs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef71a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_arr)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"running loss\")\n",
    "plt.title(f\"SGD, batch size = {batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67f247f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
